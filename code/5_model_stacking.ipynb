{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e026755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "FILE_DIR = 'sub_model_output'\n",
    "DATA_PATH = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa8710",
   "metadata": {},
   "source": [
    "# Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d976429f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_logit_list = []\n",
    "for name in os.listdir(FILE_DIR):\n",
    "    if 'mask' in name or 'ipynb_checkpoints' in name:\n",
    "        continue\n",
    "    df_logit = pickle.load(open('%s/%s'%(FILE_DIR,name), 'rb'))\n",
    "    df_logit_list.append(df_logit)\n",
    "df_train_origin_weibo = pd.read_csv('%s/train.origin_weibo.csv'%DATA_PATH, sep='\\t')\n",
    "df_train_origin_weibo.index = df_train_origin_weibo.WeiboId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "927d2324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20329\n",
      "20329\n",
      "20329\n",
      "20329\n",
      "20329\n",
      "20329\n",
      "20329\n",
      "20329\n",
      "20329\n",
      "20329\n",
      "20329\n",
      "20329\n",
      "20329\n",
      "20329\n",
      "20329\n",
      "20329\n",
      "20329\n",
      "20329\n",
      "20329\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df_logit_list)):\n",
    "    print(sum(df_logit_list[0]['WeiboId'] == df_logit_list[i]['WeiboId']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7857a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score_cch(predictions, ground_truths):\n",
    "    \n",
    "    predictions, ground_truths = np.array(predictions)-1, np.array(ground_truths)-1\n",
    "    y_pred = predictions\n",
    "    y_true = ground_truths\n",
    "\n",
    "    w=[1,10,50,100,300]\n",
    "    n = len(y_true)\n",
    "    count_r = [0 for i in range(5)]\n",
    "    count = [0 for i in range(5)]\n",
    "    for i in range(n):\n",
    "        count[y_true[i]] += 1\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            count_r[y_pred[i]] += 1\n",
    "    sum1 = sum(w[i]*count_r[i] for i in range(5))\n",
    "    sum2 = sum(w[i]*count[i] for i in range(5))\n",
    "    precision = sum1/sum2\n",
    "    return precision\n",
    "\n",
    "def df_logit2score(df_logit, ground_truths):\n",
    "    logit_name = sorted(list(set(df_logit.columns)-set(['WeiboId'])))\n",
    "    logit = np.array(df_logit[logit_name])\n",
    "    if len(logit_name) == 5:\n",
    "        predictions = np.argmax(np.array(logit), 1)\n",
    "    elif len(logit_name) == 1:\n",
    "        predictions = np.array([count2idx(num) for num in logit[:, 0]])\n",
    "    else:\n",
    "        return -1\n",
    "    cnt = 0\n",
    "    for p, g in zip(predictions, ground_truths):\n",
    "        if p+1 == g:\n",
    "            cnt += 1\n",
    "    return precision_score_cch(predictions+1, ground_truths), cnt/predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56aaea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_nn_loss_soft_ce.pickle (0.6711899287746997, 0.6136553691770378)\n",
      "df_nn_loss_jumpce_jump_ce.pickle (0.6306574272071, 0.5210290717693935)\n",
      "df_nn_loss_mse.pickle (0.6198852034664051, 0.5709085542820601)\n",
      "df_nn_loss_soft_ce_10_fold.pickle (0.6840147702729499, 0.6208372276058832)\n",
      "cch_xgb_old.pickle (0.6800944311354782, 0.5394264351419155)\n",
      "df_lgb_yjq.pickle (0.6257080995332036, 0.6677652614491613)\n",
      "df_nn_bert_loss_mse.pickle (0.6154101751960169, 0.6040139701903684)\n",
      "cch_xgb.pickle (0.6861156218681501, 0.5826651581484579)\n",
      "cch_lgb.pickle (0.6914936947655568, 0.5942249987702297)\n",
      "df_nn_bert_loss_ce.pickle (0.6610313466351538, 0.5822224408480496)\n",
      "cch_xgbReg.pickle (0.6439190529018013, 0.6626494170888878)\n",
      "df_lgb.pickle (0.6859977169316848, 0.5905848787446505)\n",
      "df_nn_bert_loss_poisson.pickle (0.6323214946058492, 0.36676668798268486)\n",
      "cch_lgbReg.pickle (0.6053132251823508, 0.6600423040975946)\n",
      "df_nn_loss_ce.pickle (0.6713078337111651, 0.5902897338777117)\n",
      "df_nn_loss_poisson.pickle (0.6175431826829805, 0.36652073392690243)\n",
      "df_nn_loss_soft_ce_24h.pickle (0.6671704423042912, 0.6117861183530916)\n",
      "cch_lgb_old.pickle (0.6831626391412233, 0.5754832997196124)\n",
      "cch_xgb_v3.pickle (0.6876671436457278, 0.582566776526145)\n"
     ]
    }
   ],
   "source": [
    "def count2idx(num):\n",
    "    if 0 <= num <= 10:\n",
    "        return 0\n",
    "    elif 10 <= num <= 50:\n",
    "        return 1\n",
    "    elif 50 <= num <= 150:\n",
    "        return 2\n",
    "    elif 150 <= num <= 300:\n",
    "        return 3\n",
    "    elif num > 300:\n",
    "        return 4\n",
    "    return 0\n",
    "    \n",
    "df_train_origin_weibo = pd.read_csv('%s/train.origin_weibo.csv'%DATA_PATH, sep='\\t')\n",
    "df_train_origin_weibo.index = df_train_origin_weibo.WeiboId\n",
    "\n",
    "\n",
    "group_truths = np.array(df_train_origin_weibo['ForwordCount'].apply(count2idx)) + 1\n",
    "df_logit_list = []\n",
    "feature_name_list = []\n",
    "for name in os.listdir(FILE_DIR):\n",
    "    if 'mask' in name or 'ipynb_checkpoints' in name:\n",
    "        continue\n",
    "\n",
    "    df_logit = pickle.load(open('%s/%s'%(FILE_DIR,name), 'rb'))\n",
    "    logit_name = list(set(df_logit.columns)-set(['WeiboId']))\n",
    "    df_logit = df_logit[logit_name]\n",
    "    logit_name = ['%s_%s'%(name.split('.')[0], f) for f in logit_name]\n",
    "    df_logit.columns = logit_name\n",
    "    feature_name_list.extend(logit_name)\n",
    "    df_logit_list.append(df_logit)\n",
    "    print(name, df_logit2score(df_logit, group_truths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "500c0489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_LSTM_yjq = pickle.load(open('%s/df_LSTM_yjq.pickle'%(FILE_DIR), 'rb'))\n",
    "# df_LSTM_yjq = pd.concat([df_LSTM_yjq[['WeiboId']],pd.get_dummies(df_LSTM_yjq['LSTM_pre'])], 1)\n",
    "# pickle.dump(df_LSTM_yjq, open('%s/df_LSTM_yjq.pickle'%(FILE_DIR), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cdeff8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.load(open('%s/cch_lgbReg.pickle'%(FILE_DIR), 'rb'))['lgbReg_cch_fowardcount'].apply(count2idx).value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72f56502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangweilin/anaconda3/envs/hyr2/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_logits = pd.concat(df_logit_list, 1)\n",
    "df_logits.index = pickle.load(open('sub_model_output/df_lgb.pickle', 'rb')).WeiboId\n",
    "feature_name_list = list(df_logits.columns)\n",
    "df_logits['label'] = df_train_origin_weibo['ForwordCount'].apply(count2idx)\n",
    "df_logits['type'] = ['train'] * 18000 + ['test'] * 2329"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f75fee24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18000, 71), (18000,), (2329, 71))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_train_test_from_df(df, feature_name_list):\n",
    "    df_train = df.iloc[:18000]\n",
    "    df_test = df.iloc[18000:]\n",
    "    train_x_fix, train_y, test_x_fix = \\\n",
    "        np.array(df_train[feature_name_list]), np.array(df_train['label']), np.array(df_test[feature_name_list])\n",
    "    \n",
    "        \n",
    "    return train_x_fix, train_y, test_x_fix\n",
    "\n",
    "train_x, train_y, test_x = \\\n",
    "get_train_test_from_df(df_logits, feature_name_list)\n",
    "\n",
    "train_x.shape, train_y.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b05db",
   "metadata": {},
   "source": [
    "# 权重赋予"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d6c1191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.893306\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.893306\n",
      "0.8933063058784899\n",
      "0.8554931282905416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangweilin/anaconda3/envs/hyr2/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/huangweilin/anaconda3/envs/hyr2/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.898343\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.898343\n",
      "0.8983427276727406\n",
      "0.8843503561153035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangweilin/anaconda3/envs/hyr2/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/huangweilin/anaconda3/envs/hyr2/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.891327\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.891327\n",
      "0.8913266382203039\n",
      "0.8772108695885034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangweilin/anaconda3/envs/hyr2/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/huangweilin/anaconda3/envs/hyr2/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.901005\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.901005\n",
      "0.9010049141200437\n",
      "0.881289544227859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangweilin/anaconda3/envs/hyr2/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/huangweilin/anaconda3/envs/hyr2/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.895779\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.895779\n",
      "0.8957786911373702\n",
      "0.8819658798365717\n",
      "0.8760438791088212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def fit_binary_model(train_x, train_y):\n",
    "    trn_data = lgb.Dataset(train_x, train_y)\n",
    "    clf = lgb.train({  \n",
    "                'boosting_type': 'gbdt',  \n",
    "                'objective': 'binary',  \n",
    "                'metric': 'auc',  \n",
    "                'num_leaves': 8,  \n",
    "                'max_depth': 3,\n",
    "                'min_data_in_leaf': 100,  \n",
    "                'learning_rate': 0.06,  \n",
    "                'feature_fraction': 0.8,  \n",
    "                'bagging_fraction': 0.8,  \n",
    "                'bagging_freq': 5,  \n",
    "                'lambda_l1': 0.5,  \n",
    "                'lambda_l2': 0.5,  \n",
    "                'min_gain_to_split': 0.2,  \n",
    "                'verbose': -1, \n",
    "                }, \n",
    "                trn_data, \n",
    "                num_boost_round = 100,\n",
    "                valid_sets = [trn_data], \n",
    "                verbose_eval = 100, \n",
    "                early_stopping_rounds = 100)\n",
    "    return clf\n",
    "\n",
    "def eval_logit(logit, label):\n",
    "    return roc_auc_score(label, logit)\n",
    "    \n",
    "def cross_validation_fit_weight(train_x, train_y):\n",
    "    n_flod = 5\n",
    "    folds = KFold(n_splits=n_flod, shuffle=True)\n",
    "    train_x = np.array(train_x)\n",
    "    train_y = np.array(train_y)\n",
    "    score_train = np.zeros((len(train_x), ))\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_x, train_y)):\n",
    "        \n",
    "        trn_x, trn_labels = train_x[trn_idx], train_y[trn_idx]\n",
    "        val_x, val_labels = train_x[val_idx], train_y[val_idx]\n",
    "        model = fit_binary_model(trn_x, trn_labels)\n",
    "        score_train[val_idx] = model.predict(val_x)\n",
    "        \n",
    "        print(eval_logit(model.predict(trn_x), trn_labels))\n",
    "        print(eval_logit(score_train[val_idx], val_labels))\n",
    "    \n",
    "    print(eval_logit(score_train, train_y))\n",
    "    return score_train\n",
    "\n",
    "def fit_train_test_weight(train_x, test_x):\n",
    "    train_y = np.array([0]*train_x.shape[0] + [1]*test_x.shape[0])\n",
    "    return cross_validation_fit_weight(np.concatenate([train_x, test_x]), train_y)\n",
    "\n",
    "\n",
    "dataset_weights = fit_train_test_weight(train_x, test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9647666",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93f9927c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangweilin/anaconda3/envs/hyr2/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/huangweilin/anaconda3/envs/hyr2/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.275751\n",
      "[200]\ttraining's multi_error: 0.258379\n",
      "[300]\ttraining's multi_error: 0.250814\n",
      "[400]\ttraining's multi_error: 0.2428\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[380]\ttraining's multi_error: 0.241968\n",
      "0\n",
      "0.7568334632284953\n",
      "0.6616012104466812\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.282398\n",
      "[200]\ttraining's multi_error: 0.266302\n",
      "[300]\ttraining's multi_error: 0.253433\n",
      "[400]\ttraining's multi_error: 0.247801\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\ttraining's multi_error: 0.247801\n",
      "1\n",
      "0.7510474577313421\n",
      "0.7940432158847577\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.281138\n",
      "[200]\ttraining's multi_error: 0.262634\n",
      "[300]\ttraining's multi_error: 0.252826\n",
      "[400]\ttraining's multi_error: 0.247253\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[395]\ttraining's multi_error: 0.246623\n",
      "2\n",
      "0.7523146406769367\n",
      "0.7241258496251138\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.275176\n",
      "[200]\ttraining's multi_error: 0.261086\n",
      "[300]\ttraining's multi_error: 0.25298\n",
      "[400]\ttraining's multi_error: 0.245779\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[395]\ttraining's multi_error: 0.245399\n",
      "3\n",
      "0.7540476031512886\n",
      "0.64981765134938\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.279777\n",
      "[200]\ttraining's multi_error: 0.264383\n",
      "[300]\ttraining's multi_error: 0.253502\n",
      "[400]\ttraining's multi_error: 0.243717\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\ttraining's multi_error: 0.243717\n",
      "4\n",
      "0.7556691403038827\n",
      "0.7429233144621719\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.278422\n",
      "[200]\ttraining's multi_error: 0.258023\n",
      "[300]\ttraining's multi_error: 0.251523\n",
      "[400]\ttraining's multi_error: 0.240186\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\ttraining's multi_error: 0.240186\n",
      "5\n",
      "0.759005057568419\n",
      "0.6988094357760852\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.276028\n",
      "[200]\ttraining's multi_error: 0.258537\n",
      "[300]\ttraining's multi_error: 0.250163\n",
      "[400]\ttraining's multi_error: 0.243577\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[398]\ttraining's multi_error: 0.243298\n",
      "6\n",
      "0.7553026268461952\n",
      "0.7309993809753078\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.273774\n",
      "[200]\ttraining's multi_error: 0.254549\n",
      "[300]\ttraining's multi_error: 0.251775\n",
      "[400]\ttraining's multi_error: 0.23991\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[396]\ttraining's multi_error: 0.2398\n",
      "7\n",
      "0.758830002089282\n",
      "0.652424860984022\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.276048\n",
      "[200]\ttraining's multi_error: 0.256765\n",
      "[300]\ttraining's multi_error: 0.248322\n",
      "[400]\ttraining's multi_error: 0.240916\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\ttraining's multi_error: 0.240916\n",
      "8\n",
      "0.7573265800730424\n",
      "0.7530929892244247\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.276453\n",
      "[200]\ttraining's multi_error: 0.256858\n",
      "[300]\ttraining's multi_error: 0.251591\n",
      "[400]\ttraining's multi_error: 0.244795\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\ttraining's multi_error: 0.244795\n",
      "9\n",
      "0.7541333555407506\n",
      "0.7264719517848864\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.276347\n",
      "[200]\ttraining's multi_error: 0.258842\n",
      "[300]\ttraining's multi_error: 0.251123\n",
      "[400]\ttraining's multi_error: 0.23933\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\ttraining's multi_error: 0.23933\n",
      "10\n",
      "0.7586781790251668\n",
      "0.6354605036219386\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.274968\n",
      "[200]\ttraining's multi_error: 0.261766\n",
      "[300]\ttraining's multi_error: 0.252446\n",
      "[400]\ttraining's multi_error: 0.245227\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\ttraining's multi_error: 0.245227\n",
      "11\n",
      "0.7531757845726821\n",
      "0.7348321722609247\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.27823\n",
      "[200]\ttraining's multi_error: 0.254759\n",
      "[300]\ttraining's multi_error: 0.246421\n",
      "[400]\ttraining's multi_error: 0.241184\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[395]\ttraining's multi_error: 0.240044\n",
      "12\n",
      "0.7586922082211413\n",
      "0.6302489313552929\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.276515\n",
      "[200]\ttraining's multi_error: 0.258006\n",
      "[300]\ttraining's multi_error: 0.251454\n",
      "[400]\ttraining's multi_error: 0.242071\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\ttraining's multi_error: 0.242071\n",
      "13\n",
      "0.7572111076261314\n",
      "0.5974897477320741\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.271997\n",
      "[200]\ttraining's multi_error: 0.259452\n",
      "[300]\ttraining's multi_error: 0.253182\n",
      "[400]\ttraining's multi_error: 0.245076\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\ttraining's multi_error: 0.245076\n",
      "14\n",
      "0.7533373718337035\n",
      "0.6680510412904779\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.281004\n",
      "[200]\ttraining's multi_error: 0.26139\n",
      "[300]\ttraining's multi_error: 0.250505\n",
      "[400]\ttraining's multi_error: 0.243771\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[399]\ttraining's multi_error: 0.242542\n",
      "15\n",
      "0.7557523561001014\n",
      "0.7198577134298553\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.283526\n",
      "[200]\ttraining's multi_error: 0.260808\n",
      "[300]\ttraining's multi_error: 0.255502\n",
      "[400]\ttraining's multi_error: 0.247085\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[398]\ttraining's multi_error: 0.246873\n",
      "16\n",
      "0.751625556625766\n",
      "0.7793558711742349\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.273844\n",
      "[200]\ttraining's multi_error: 0.25826\n",
      "[300]\ttraining's multi_error: 0.251053\n",
      "[400]\ttraining's multi_error: 0.243995\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[392]\ttraining's multi_error: 0.24297\n",
      "17\n",
      "0.7555653004563966\n",
      "0.7312005277044855\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.277451\n",
      "[200]\ttraining's multi_error: 0.256071\n",
      "[300]\ttraining's multi_error: 0.248658\n",
      "[400]\ttraining's multi_error: 0.241965\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\ttraining's multi_error: 0.241965\n",
      "18\n",
      "0.7569448298058888\n",
      "0.7211433046202036\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.276905\n",
      "[200]\ttraining's multi_error: 0.257555\n",
      "[300]\ttraining's multi_error: 0.249969\n",
      "[400]\ttraining's multi_error: 0.239472\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\ttraining's multi_error: 0.239472\n",
      "19\n",
      "0.7595702757172612\n",
      "0.7455651874390034\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.275584\n",
      "[200]\ttraining's multi_error: 0.256887\n",
      "[300]\ttraining's multi_error: 0.249421\n",
      "[400]\ttraining's multi_error: 0.242262\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[382]\ttraining's multi_error: 0.241789\n",
      "20\n",
      "0.7570478275747906\n",
      "0.7095671153146599\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.27678\n",
      "[200]\ttraining's multi_error: 0.256882\n",
      "[300]\ttraining's multi_error: 0.250733\n",
      "[400]\ttraining's multi_error: 0.242856\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\ttraining's multi_error: 0.242856\n",
      "21\n",
      "0.7558108670688195\n",
      "0.618657937806874\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.274214\n",
      "[200]\ttraining's multi_error: 0.260703\n",
      "[300]\ttraining's multi_error: 0.251029\n",
      "[400]\ttraining's multi_error: 0.245857\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[397]\ttraining's multi_error: 0.245359\n",
      "22\n",
      "0.7541642316478809\n",
      "0.6250884073812126\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.277177\n",
      "[200]\ttraining's multi_error: 0.254048\n",
      "[300]\ttraining's multi_error: 0.250514\n",
      "[400]\ttraining's multi_error: 0.239449\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\ttraining's multi_error: 0.239449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "0.7588181087387016\n",
      "0.6473609306354805\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.279861\n",
      "[200]\ttraining's multi_error: 0.260179\n",
      "[300]\ttraining's multi_error: 0.251431\n",
      "[400]\ttraining's multi_error: 0.240715\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[390]\ttraining's multi_error: 0.240426\n",
      "24\n",
      "0.7583236909211487\n",
      "0.6665103556076593\n",
      "train score 0.755969, val score 0.694380\n"
     ]
    }
   ],
   "source": [
    "SEED = 666 \n",
    "# import random\n",
    "# SEED = random.randint(0, 1314)\n",
    "# print(SEED)\n",
    "\n",
    "params = {  \n",
    "    'boosting_type': 'gbdt',  \n",
    "    'objective': 'multiclass',  \n",
    "    'num_class': 5,  \n",
    "    'metric': 'multi_error',  \n",
    "    'num_leaves': 16,  \n",
    "    'max_depth': 4,\n",
    "    'min_data_in_leaf': 100,  \n",
    "    'learning_rate': 0.005,  \n",
    "    'feature_fraction': 0.8,  \n",
    "    'bagging_fraction': 0.8,  \n",
    "    'bagging_freq': 5,  \n",
    "    'lambda_l1': 0.5,  \n",
    "    'lambda_l2': 0.5,  \n",
    "    'min_gain_to_split': 0.2,  \n",
    "    'verbose': -1, \n",
    "    'feature_fraction_seed':SEED,\n",
    "    'bagging_seed':SEED,\n",
    "\n",
    "} \n",
    "\n",
    "def precision_score_cch(predictions, ground_truths):\n",
    "    \n",
    "    predictions, ground_truths = np.array(predictions)-1, np.array(ground_truths)-1\n",
    "    y_pred = predictions\n",
    "    y_true = ground_truths\n",
    "\n",
    "    w=[1,10,50,100,300]\n",
    "    n = len(y_true)\n",
    "    count_r = [0 for i in range(5)]\n",
    "    count = [0 for i in range(5)]\n",
    "    for i in range(n):\n",
    "        count[y_true[i]] += 1\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            count_r[y_pred[i]] += 1\n",
    "    sum1 = sum(w[i]*count_r[i] for i in range(5))\n",
    "    sum2 = sum(w[i]*count[i] for i in range(5))\n",
    "    precision = sum1/sum2\n",
    "    return precision\n",
    "\n",
    "def fit_lgb(train_x, train_y, sub_dataset_weights):\n",
    "    class_weights = [1,10,50,100,300]\n",
    "#     sub_dataset_weights = [1] * len(sub_dataset_weights)\n",
    "    trn_data = lgb.Dataset(train_x, train_y,\n",
    "                           weight=[class_weights[int(y)]*sub_dataset_weights[i] for i, y in enumerate(train_y)])\n",
    "    clf = lgb.train(params, \n",
    "                    trn_data, \n",
    "                    num_boost_round = 400,\n",
    "                    valid_sets = [trn_data], \n",
    "                    verbose_eval = 100, \n",
    "                    early_stopping_rounds = 100\n",
    "                   )\n",
    "    return clf\n",
    "\n",
    "def eval_logit(logit, label):\n",
    "    prediction = np.argmax(logit, 1)\n",
    "    return precision_score_cch(prediction+1, np.array(label).astype('int')+1)\n",
    "    \n",
    "def cross_validation_lgb(train_x, train_y, test_x, dataset_weights):\n",
    "    n_flod = 25\n",
    "    folds = KFold(n_splits=n_flod, shuffle=True, random_state=SEED)\n",
    "    train_x = np.array(train_x)\n",
    "    train_y = np.array(train_y)\n",
    "    score_train = np.zeros((len(train_x), 5))\n",
    "    score_test = np.zeros((len(test_x), 5))\n",
    "    train_score_list = []\n",
    "    for flod, (trn_idx, val_idx) in enumerate(folds.split(train_x, train_y)):\n",
    "        \n",
    "        trn_x, trn_labels = train_x[trn_idx], train_y[trn_idx]\n",
    "        val_x, val_labels = train_x[val_idx], train_y[val_idx]\n",
    "        model = fit_lgb(trn_x, trn_labels, dataset_weights[trn_idx])\n",
    "        score_train[val_idx] = model.predict(val_x)\n",
    "        score_test += model.predict(test_x)/n_flod\n",
    "        \n",
    "        train_score = eval_logit(model.predict(trn_x), trn_labels)\n",
    "        print(flod)\n",
    "        print(train_score)\n",
    "        train_score_list.append(train_score)\n",
    "        print(eval_logit(score_train[val_idx], val_labels))\n",
    "    \n",
    "    prediction = np.argmax(score_train, 1)\n",
    "    val_score = precision_score_cch(prediction+1, np.array(train_y).astype('int')+1)\n",
    "    \n",
    "    print('train score %f, val score %f'%(np.mean(train_score_list), val_score))\n",
    "\n",
    "    return score_train, score_test\n",
    "\n",
    "score_train, score_test = cross_validation_lgb(train_x, train_y, test_x, dataset_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91ebf4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.456611\n",
      "0    0.319444\n",
      "2    0.163167\n",
      "4    0.033222\n",
      "3    0.027556\n",
      "dtype: float64\n",
      "baseline 0.699173593581684\n"
     ]
    }
   ],
   "source": [
    "# sub_feature_name = ['lgb_hyr_0', 'lgb_hyr_1', 'lgb_hyr_2', 'lgb_hyr_3','lgb_hyr_4']\n",
    "# sub_feature_name =  ['nn_hyr_loss_ce_0', 'nn_hyr_loss_ce_1', 'nn_hyr_loss_ce_2', 'nn_hyr_loss_ce_3', 'nn_hyr_loss_ce_4']\n",
    "    \n",
    "# prediction = np.argmax(np.array(df_logits[sub_feature_name]), 1)\n",
    "prediction = np.argmax(score_train, 1)\n",
    "print(pd.Series(prediction).value_counts(1))\n",
    "print('baseline', precision_score_cch(prediction+1, np.array(train_y).astype('int')+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b084b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediciton_test = np.argmax(score_test, 1)\n",
    "df_submit = pd.DataFrame({\n",
    "    'WeiboId':list(df_logits.query('type==\"test\"').index),\n",
    "    'ForwardScale':prediciton_test+1\n",
    "})\n",
    "SUBMIT_PATH = '.'\n",
    "df_submit.to_csv('%s/submission.csv'%SUBMIT_PATH, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "541eecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit = pd.read_csv('%s/submission.csv'%SUBMIT_PATH,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "14ffee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit_old = pd.read_csv('submit/submission_12_12.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a8a22f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9540575354229283\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for a, b in zip(df_submit['ForwardScale'], df_submit_old['ForwardScale']):\n",
    "    if a==b:\n",
    "        cnt += 1\n",
    "        \n",
    "print(cnt/2329)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b6dc97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyr2",
   "language": "python",
   "name": "hyr2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
