{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cch/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.8' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/dask/config.py:161: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/dask/dataframe/utils.py:13: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:31: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "from gensim.models.word2vec import Word2Vec \n",
    "import logging\n",
    "LOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)\n",
    "import jieba\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import time\n",
    "from datetime import datetime\n",
    "from category_encoders import TargetEncoder\n",
    "import IPython.display as ipd\n",
    "from collections import *\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/data/cch/weibo'\n",
    "VAR_PATH = '/data/cch/hyr/weibo/var'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_origin_weibo = pd.read_csv('%s/test.origin_weibo.csv'%DATA_PATH, sep='\\t')\n",
    "df_test_repost = pd.read_csv('%s/test.repost.csv'%DATA_PATH, sep='\\t')\n",
    "df_train_origin_weibo = pd.read_csv('%s/train.origin_weibo.csv'%DATA_PATH, sep='\\t')\n",
    "df_train_repost = pd.read_csv('%s/train.repost.csv'%DATA_PATH, sep='\\t')\n",
    "# df_user_profile = pd.read_csv('%s/user_profile.csv'%DATA_PATH, sep='\\t')\n",
    "df_user_profile_agg = pickle.load(open('%s/df_user_profile_agg.pickle'%VAR_PATH, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def count2idx(num):\n",
    "    if 0 <= num <= 10:\n",
    "        return 0\n",
    "    elif 11 <= num <= 50:\n",
    "        return 1\n",
    "    elif 51 <= num <= 150:\n",
    "        return 2\n",
    "    elif 151 <= num <= 300:\n",
    "        return 3\n",
    "    elif num > 300:\n",
    "        return 4\n",
    "    if num < 0:\n",
    "        return 0\n",
    "df_train_origin_weibo['label'] = df_train_origin_weibo['ForwordCount'].apply(count2idx)\n",
    "\n",
    "df_train_origin_weibo['type'] = 'train'\n",
    "df_test_origin_weibo['type'] = 'test'\n",
    "df_origin_weibo = pd.concat([df_train_origin_weibo, df_test_origin_weibo])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_wv(texts):\n",
    "    text_list = []\n",
    "    for text in tqdm(texts):\n",
    "        text_list.append(list(jieba.cut(text)))\n",
    "    wv_model= Word2Vec(text_list, min_count=1, vector_size = 10, sg = 1)\n",
    "    text_embed_list = []\n",
    "    \n",
    "    for sentence in text_list:\n",
    "        word_embed_list = []\n",
    "        for word in sentence:\n",
    "            word_embed_list.append(wv_model.wv[word])\n",
    "        text_embed_list.append(np.array(word_embed_list).mean(0))\n",
    "    \n",
    "    return np.array(text_embed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfidf_svd_matrix(texts, n_output):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    for text in texts:\n",
    "        words = list(jieba.cut(str(text)))\n",
    "        use_words = []\n",
    "        for word in words:\n",
    "            use_words.append(word)\n",
    "        corpus.append(' '.join(use_words))\n",
    "    tfidf_vec = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vec.fit_transform(corpus)\n",
    "    svd = TruncatedSVD(n_components=n_output, n_iter=7, random_state=42)\n",
    "    tf_idf_svd = svd.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    return tf_idf_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_condition(text, condition_list):\n",
    "\n",
    "    \n",
    "    ret = {}\n",
    "    for item in condition_list:\n",
    "        ret[item['name']] = 0\n",
    "        for or_condition in item['condition'].split('|'):\n",
    "            and_ret = True\n",
    "            for and_condition in or_condition.split('&'):\n",
    "                target_str = and_condition.strip()\n",
    "                if target_str not in text:\n",
    "                    and_ret = False\n",
    "            if and_ret:\n",
    "                ret[item['name']] = 1\n",
    "                break\n",
    "    \n",
    "    return pd.Series(ret)\n",
    "\n",
    "\n",
    "def get_text_hard_feature():\n",
    "    global df_weibo\n",
    "    \n",
    "    se_text_len = df_weibo['WeiboText'].apply(len)\n",
    "    se_text_len.name = 'weibotext_len'\n",
    "    \n",
    "    '''\n",
    "        转发&点赞 疫情 特朗普|总统 视频|链接\n",
    "        粉|饭 中国&金牌 台湾 历史 发展|经济 推荐\n",
    "        东京&奥运\n",
    "    '''\n",
    "    condition_list = [\n",
    "        {'name':'转发&点赞','condition':'转发&点赞'},\n",
    "        {'name':'疫情','condition':'疫情'},\n",
    "        {'name':'特朗普|总统','condition':'特朗普|总统'},\n",
    "        {'name':'视频|链接','condition':'视频|链接'},\n",
    "        {'name':'粉|饭','condition':'粉|饭'},\n",
    "        {'name':'中国&金牌','condition':'中国&金牌'},\n",
    "        {'name':'台湾','condition':'台湾'},\n",
    "        {'name':'历史','condition':'历史'},\n",
    "        {'name':'发展|经济','condition':'发展|经济'},\n",
    "        {'name':'推荐','condition':'推荐'},\n",
    "        {'name':'东京&奥运','condition':'东京&奥运'},\n",
    "    ]\n",
    "    \n",
    "    df_condition = df_weibo['WeiboText'].apply(lambda x : get_text_condition(x, condition_list))\n",
    "    return pd.concat([se_text_len,df_condition], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     19
    ]
   },
   "outputs": [],
   "source": [
    "def precision_score_cch(predictions, ground_truths):\n",
    "    \n",
    "    predictions, ground_truths = np.array(predictions)-1, np.array(ground_truths)-1\n",
    "    y_pred = predictions\n",
    "    y_true = ground_truths\n",
    "\n",
    "    w=[1,10,50,100,300]\n",
    "    n = len(y_true)\n",
    "    count_r = [0 for i in range(5)]\n",
    "    count = [0 for i in range(5)]\n",
    "    for i in range(n):\n",
    "        count[y_true[i]] += 1\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            count_r[y_pred[i]] += 1\n",
    "    sum1 = sum(w[i]*count_r[i] for i in range(5))\n",
    "    sum2 = sum(w[i]*count[i] for i in range(5))\n",
    "    precision = sum1/sum2\n",
    "    return precision\n",
    "\n",
    "def get_text_stacking_feature(df_weibo):\n",
    "    df_train = df_weibo.query('type==\"train\"')\n",
    "    df_test = df_weibo.query('type==\"test\"')\n",
    "    \n",
    "    hidden_feature_list = [\n",
    "        'weibotext_wv_embed', 'weibotext_tfidf',#微博内容词向量/tfidf+svd降维\n",
    "        'user_intro_wv_embed', 'user_intro_tfidf'#用户个性签名词向量/tfidf+svd降维\n",
    "    ]\n",
    "    train_x_list, test_x_list = [], []\n",
    "    \n",
    "    for f in hidden_feature_list:\n",
    "        hidden_train = np.array(list(df_train[f]))\n",
    "        hidden_test = np.array(list(df_test[f]))\n",
    "        train_x_list.append(hidden_train)\n",
    "        test_x_list.append(hidden_test)\n",
    "        \n",
    "    train_x = np.concatenate(train_x_list, 1)\n",
    "    test_x = np.concatenate(test_x_list, 1)\n",
    "    \n",
    "#     print(train_x.shape, test_x.shape)\n",
    "    train_y = np.array(df_train['label'])\n",
    "    \n",
    "    SEED = 42\n",
    "    params = {  \n",
    "        'boosting_type': 'gbdt',  \n",
    "        'objective': 'multiclass',  \n",
    "        'num_class': 5,  \n",
    "        'metric': 'multi_error',  \n",
    "        'num_leaves': 8,  \n",
    "        'max_depth': 3,\n",
    "        'min_data_in_leaf': 100,  \n",
    "        'learning_rate': 0.01,  \n",
    "        'feature_fraction': 0.8,  \n",
    "        'bagging_fraction': 0.8,  \n",
    "        'bagging_freq': 5,  \n",
    "        'lambda_l1': 0.5,  \n",
    "        'lambda_l2': 0.5,  \n",
    "        'min_gain_to_split': 0.2,  \n",
    "        'verbose': -1, \n",
    "\n",
    "        'feature_fraction_seed':SEED,\n",
    "        'bagging_seed':SEED,\n",
    "    } \n",
    "\n",
    "    def fit_lgb(train_x, train_y):\n",
    "        class_weights = [1,10,50,100,300]\n",
    "        trn_data = lgb.Dataset(train_x, train_y, weight=[class_weights[int(y)] for y in train_y])\n",
    "        clf = lgb.train(params, \n",
    "                        trn_data, \n",
    "                        num_boost_round = 400,\n",
    "                        valid_sets = [trn_data], \n",
    "                        verbose_eval = 100, \n",
    "                        early_stopping_rounds = 100\n",
    "                       )\n",
    "        return clf\n",
    "\n",
    "    def eval_logit(logit, label):\n",
    "        prediction = np.argmax(logit, 1)\n",
    "        return precision_score_cch(prediction+1, np.array(label).astype('int')+1)\n",
    "\n",
    "    def cross_validation_lgb(train_x, train_y, test_x):\n",
    "        n_flod = 5\n",
    "        folds = KFold(n_splits=n_flod, shuffle=True,random_state=SEED)\n",
    "        train_x = np.array(train_x)\n",
    "        train_y = np.array(train_y)\n",
    "        score_train = np.zeros((len(train_x), 5))\n",
    "        score_test = np.zeros((len(test_x), 5))\n",
    "        trainset_score_list = []\n",
    "        for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_x, train_y)):\n",
    "\n",
    "            trn_x, trn_labels = train_x[trn_idx], train_y[trn_idx]\n",
    "            val_x, val_labels = train_x[val_idx], train_y[val_idx]\n",
    "            model = fit_lgb(trn_x, trn_labels)\n",
    "            score_train[val_idx] = model.predict(val_x)\n",
    "            score_test += model.predict(test_x)/n_flod\n",
    "\n",
    "            train_score = eval_logit(model.predict(trn_x), trn_labels)\n",
    "            trainset_score_list.append(train_score)\n",
    "            \n",
    "        print(eval_logit(score_train, train_y))\n",
    "        return score_train, score_test\n",
    "    \n",
    "    score_train, score_test = cross_validation_lgb(train_x, train_y, test_x)\n",
    "    \n",
    "    return pd.DataFrame(np.concatenate([score_train, score_test]), columns=['text_stacking_0','text_stacking_1',\n",
    "                                                                           'text_stacking_2','text_stacking_3',\n",
    "                                                                           'text_stacking_4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时间特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def apply_weibo_creatime(x):\n",
    "    date_obj = datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "    return pd.Series({\n",
    "        'post_day':date_obj.day,\n",
    "        'post_weekday':date_obj.weekday(),\n",
    "        'post_month':date_obj.month, \n",
    "        'post_hour':date_obj.hour, \n",
    "        'post_minute':date_obj.minute,\n",
    "        'post_year':date_obj.year,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用户id注入target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.get_dummies(df_weibo.query('UserId == \"65ba57e1fd5daa6042ca6c12f3cc9c1b031aacbe\"')['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def set_userid_target_encode(df_weibo):\n",
    "    df_weibo_train = df_weibo.query('type == \"train\"')\n",
    "    df_weibo_test = df_weibo.query('type == \"test\"')  \n",
    "    \n",
    "    \n",
    "    n_flod = 5\n",
    "    folds = KFold(n_splits=n_flod, shuffle=True)\n",
    "    train_x = np.array(df_weibo_train['UserId'])\n",
    "    train_forword_cnt = np.array(df_weibo_train['ForwordCount'])\n",
    "    train_labels = np.array(np.array(pd.get_dummies(df_weibo_train['label'])))\n",
    "    test_x = np.array(df_weibo_test['UserId'])\n",
    "    \n",
    "    target_encode_train = np.zeros((len(train_x), 6))\n",
    "    target_encode_test = np.zeros((len(test_x), 6))\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_x, train_labels)):\n",
    "        \n",
    "        trn_x, trn_cnt, trn_labels = train_x[trn_idx], train_forword_cnt[trn_idx], train_labels[trn_idx]\n",
    "        val_x, val_cnt, val_labels = train_x[val_idx], train_forword_cnt[val_idx], train_labels[val_idx]\n",
    "        \n",
    "        for i in range(5):\n",
    "            encoder = TargetEncoder()\n",
    "            encoder.fit(trn_x, trn_labels[:, i])\n",
    "            target_encode_train[val_idx, i:i+1] = encoder.transform(val_x)\n",
    "            target_encode_test[:, i:i+1] += encoder.transform(test_x)/n_flod\n",
    "\n",
    "        \n",
    "        encoder = TargetEncoder()\n",
    "        encoder.fit(trn_x, trn_cnt)\n",
    "        target_encode_train[val_idx, 5:6] = encoder.transform(val_x)\n",
    "        target_encode_test[:, 5:6] += encoder.transform(test_x)/n_flod\n",
    "    \n",
    "    column_names = ['target_encode_0', 'target_encode_1', 'target_encode_2', 'target_encode_3', 'target_encode_4', \n",
    "                    'target_encode_cnt']\n",
    "    \n",
    "    target_encode_score = np.concatenate([target_encode_train, target_encode_test])\n",
    "    for i in range(6):\n",
    "        df_weibo[column_names[i]] = list(target_encode_score[:, i])\n",
    "    return df_weibo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转发特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def apply_pass_time(a_row):\n",
    "    global mp_WeiboId_create_time\n",
    "    repost_date_obj = datetime.strptime(a_row['RepostDate'], '%Y-%m-%d %H:%M:%S')\n",
    "    post_date_obj = datetime.strptime(mp_WeiboId_create_time[a_row['OriginWeiboId']], '%Y-%m-%d %H:%M:%S')\n",
    "    return (repost_date_obj-post_date_obj).seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1h内转发特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_repost_user_set(df_repost_1h, df_user_profile_agg, need_weibo_id_set):\n",
    "    '''\n",
    "    转发用户、转发文本集合\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    df_user_profile_dense = df_user_profile_agg[['follow_mean', 'follow_median', 'follow_max', 'follow_min', 'follower_mean','follower_median',\n",
    "                 'follower_max', 'follower_min']]\n",
    "    user_profile_mean = df_user_profile_dense.mean()\n",
    "    user_profile_std = df_user_profile_dense.std()\n",
    "\n",
    "    df_user_profile_dense = copy.deepcopy(df_user_profile_dense)\n",
    "    df_user_profile_dense['norm_dense'] = list(np.array((df_user_profile_dense - user_profile_mean)/user_profile_std))\n",
    "    mp_userid_dense = df_user_profile_dense['norm_dense'].to_dict()\n",
    "    \n",
    "    repost_id_freq_set = set(df_repost_1h['RepostUserId'].value_counts()[df_repost_1h['RepostUserId'].value_counts() > 10].index)\n",
    "    mp_repost_userid_idx_freq = dict([(userid,i+1) for i, userid in enumerate(set(repost_id_freq_set))])\n",
    "    \n",
    "    mp_repost_userid_idx = dict([(userid,i+1) for i, userid in enumerate(set(df_repost_1h['RepostUserId']))])\n",
    "    default_idx = len(mp_repost_userid_idx_freq)+1\n",
    "    \n",
    "    df_repost_1h['repost_text_wv'] = list(generate_wv(df_repost_1h['RepostWeiboText'].fillna(' ')))\n",
    "    df_repost_1h['user_dense'] = list(df_repost_1h['RepostUserId'].map(mp_userid_dense))\n",
    "    \n",
    "    ret_dict_list = []\n",
    "    \n",
    "    for id_name, df in tqdm(df_repost_1h.groupby('OriginWeiboId')):\n",
    "        ret_dict_list.append({\n",
    "            'WeiboId':id_name,\n",
    "            'repost_weibo_repost_text_wv':[np.zeros((10,))] + list(df['repost_text_wv']),\n",
    "            'repost_weibo_repost_user_id':[0]+list(df['RepostUserId'].map(mp_repost_userid_idx)),\n",
    "            'repost_weibo_repost_user_id_freq':[0]+list(df['RepostUserId'].apply(lambda x:mp_repost_userid_idx_freq.get(x,default_idx))),\n",
    "            'repost_weibo_set_len' : df['RepostUserId'].shape[0]+1,\n",
    "            'repost_weibo_set_user_dense':[np.zeros((8,))]+list(df['user_dense'])\n",
    "        })\n",
    "        \n",
    "    for left_id in set(need_weibo_id_set) - set(df_repost_1h['OriginWeiboId']):\n",
    "        ret_dict_list.append({\n",
    "            'WeiboId':left_id,\n",
    "            'repost_weibo_repost_text_wv':[np.zeros((10,))],\n",
    "            'repost_weibo_repost_user_id':[0],\n",
    "            'repost_weibo_repost_user_id_freq':[0],\n",
    "            'repost_weibo_set_len' : 1,\n",
    "            'repost_weibo_set_user_dense':[np.zeros((8,))]\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(ret_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_1h_features(df_user_profile_agg, df_repost_1h, name):\n",
    "    df_repost_1h = df_repost_1h.fillna('')\n",
    "    user_follow_feature_list = ['follow_mean', 'follow_median', 'follow_max', 'follow_min', 'follower_mean','follower_median',\n",
    "                 'follower_max', 'follower_min']\n",
    "    user_follow_dict = df_user_profile_agg[user_follow_feature_list].to_dict()\n",
    "    user_attribute_dict = df_user_profile_agg[['Verified', 'Gender']].to_dict()\n",
    "    \n",
    "    ret_dict_list = []\n",
    "    for id_name, df in tqdm(df_repost_1h.groupby({'weibo':'OriginWeiboId','userid':'OriginUserId'}[name])):\n",
    "        df15 = df.query('pass_time < 15*60')\n",
    "        df30 = df.query('pass_time < 30*60')\n",
    "        df45 = df.query('pass_time < 45*60')\n",
    "        df60 = df.query('pass_time < 60*60')\n",
    "        \n",
    "        \n",
    "        repost_other_cnt = []\n",
    "        for repost_text in df['RepostWeiboText']:\n",
    "            repost_text = str(repost_text)\n",
    "            repost_other_cnt.append(repost_text.count('//@'))\n",
    "\n",
    "        repost_other_cnt = np.array(repost_other_cnt)\n",
    "        repost_text_len = np.array(df['RepostWeiboText'].apply(len))\n",
    "\n",
    "        ret_dict = {\n",
    "            {'weibo':'WeiboId','userid':'UserId'}[name]:id_name,\n",
    "            'repost_%s_pass_time_mean'%name:df['pass_time'].mean(),\n",
    "            'repost_%s_pass_time_median'%name:df['pass_time'].median(),\n",
    "            'repost_%s_pass_time_min'%name:df['pass_time'].min(),\n",
    "            'repost_%s_pass_time_max'%name:df['pass_time'].max(),\n",
    "            'repost_%s_cnt_15_mins'%name:df15.shape[0],\n",
    "            'repost_%s_cnt_30_mins'%name:df30.shape[0],\n",
    "            'repost_%s_cnt_45_mins'%name:df45.shape[0],\n",
    "            'repost_%s_cnt_60_mins'%name:df60.shape[0],\n",
    "            'repost_%s_cnt_15_30_mins'%name:df30.shape[0]-df15.shape[0],\n",
    "            'repost_%s_cnt_30_45_mins'%name:df45.shape[0]-df30.shape[0],\n",
    "            'repost_%s_cnt_45_60_mins'%name:df60.shape[0]-df45.shape[0],\n",
    "            \n",
    "            'repost_%s_unique_repost_userid'%name:len(df['RepostUserId'].unique()),\n",
    "            'repost_%s_repeat_repost_cnt'%name:df60.shape[0]-len(df['RepostUserId'].unique()),\n",
    "            'repost_%s_max_user_repost'%name:max(df['RepostUserId'].value_counts()),\n",
    "            'repost_%s_other_cnt_max'%name:repost_other_cnt.max(),\n",
    "            'repost_%s_other_cnt_min'%name:repost_other_cnt.min(),\n",
    "            'repost_%s_other_cnt_mean'%name:repost_other_cnt.mean(),\n",
    "            'repost_%s_other_cnt_std'%name:repost_other_cnt.std(),\n",
    "            'repost_%s_other_cnt_0'%name:len(repost_other_cnt[repost_other_cnt>0]),\n",
    "\n",
    "            'repost_%s_text_len_max'%name:repost_text_len.max(),\n",
    "            'repost_%s_text_len_min'%name:repost_text_len.min(),\n",
    "            'repost_%s_text_len_mean'%name:repost_text_len.mean(),\n",
    "            'repost_%s_text_len_std'%name:repost_text_len.std(),\n",
    "        }\n",
    "        \n",
    "        #转发用户的特征聚合\n",
    "        mp_user_follow_feature = defaultdict(list)\n",
    "        mp_user_attribute_feature = defaultdict(list)\n",
    "\n",
    "        for repost_userid in df['RepostUserId']:\n",
    "            for user_follow_feature in user_follow_dict:\n",
    "                mp_userid_feature_v = user_follow_dict[user_follow_feature]\n",
    "                if repost_userid in mp_userid_feature_v:\n",
    "                    mp_user_follow_feature[user_follow_feature].append(mp_userid_feature_v[repost_userid])\n",
    "\n",
    "            for user_attribute_feature in user_attribute_dict:\n",
    "                if repost_userid in user_attribute_dict[user_attribute_feature]:\n",
    "                    mp_user_attribute_feature[user_attribute_feature].append(\\\n",
    "                        user_attribute_dict[user_attribute_feature][repost_userid])\n",
    "\n",
    "        \n",
    "        for feature_name in mp_user_follow_feature:\n",
    "            ret_dict['repost_%s_%s_mean'%(name,feature_name)] = np.array(mp_user_follow_feature[feature_name]).mean()\n",
    "            ret_dict['repost_%s_%s_sum'%(name,feature_name)] = np.array(mp_user_follow_feature[feature_name]).sum()\n",
    "            ret_dict['repost_%s_%s_min'%(name,feature_name)] = np.array(mp_user_follow_feature[feature_name]).min()\n",
    "            ret_dict['repost_%s_%s_max'%(name,feature_name)] = np.array(mp_user_follow_feature[feature_name]).max()\n",
    "            ret_dict['repost_%s_%s_std'%(name,feature_name)] = np.array(mp_user_follow_feature[feature_name]).std()\n",
    "\n",
    "                \n",
    "        for feature_name in mp_user_attribute_feature:\n",
    "            ret_dict['repost_%s_%s'%(name,feature_name)] = np.mean(mp_user_attribute_feature[feature_name])\n",
    "\n",
    "        ret_dict_list.append(ret_dict)\n",
    "        \n",
    "    return pd.DataFrame(ret_dict_list)\n",
    "\n",
    "# generate_1h_features(df_user_profile_agg, df_repost_1h.iloc[:1000], 'weibo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转发特征分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(set(df_repost_1h['CurrentUserId']) & set(df_repost_1h['RepostUserId'])),\\\n",
    "# len(set(df_repost_1h['CurrentUserId'])), len(set(df_repost_1h['RepostUserId']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先放弃，转发图本身是一个子图采用有偏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def generate_1h_user_id_grapg_feature()\n",
    "#     df_repost_1h[['CurrentUserId', 'RepostUserId']]\n",
    "#     graph = defaultdict(list)\n",
    "    \n",
    "# df_repost_graph_feature = generate_1h_user_id_grapg_feature()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_se(se1, se2):\n",
    "    df_tmp = pd.concat([se1, se2], 1)\n",
    "    name1, name2 = se1.name, se2.name\n",
    "    se_cross = df_tmp.apply(lambda x:str(x[name1])+'_'+str(x[name2]), 1)\n",
    "    mp_id2idx = dict([(item, i) for i, item in enumerate(set(se_cross))])\n",
    "    se_cross = se_cross.map(mp_id2idx)\n",
    "    se_cross.name = 'cross_%s_%s'%(name1, name2)\n",
    "    return se_cross\n",
    "    \n",
    "def get_cross_feature():\n",
    "    global df_weibo\n",
    "    \n",
    "    mp_userid_idx = dict([(userid, i) for i, userid in enumerate(set(df_weibo['UserId']))])\n",
    "\n",
    "    se_userid_idx = df_weibo['UserId'].map(mp_userid_idx)#1\n",
    "    se_userid_idx.name = 'userid_idx'\n",
    "    \n",
    "    se_cross_userid_post_hour = cross_se(se_userid_idx, df_weibo['post_hour'])#2\n",
    "    se_cross_userid_post_weekday = cross_se(se_userid_idx, df_weibo['post_weekday'])#3\n",
    "    \n",
    "    se_repost_1h_cnt = df_weibo['repost_weibo_cnt_60_mins'].map(count2idx)\n",
    "    se_repost_1h_cnt.name = 'repost_1h_cnt_idx'\n",
    "    \n",
    "    se_cross_repost_hour = cross_se(se_repost_1h_cnt, df_weibo['post_hour'])#4\n",
    "    se_cross_repost_weekday = cross_se(se_repost_1h_cnt, df_weibo['post_weekday'])#5\n",
    "    \n",
    "    \n",
    "\n",
    "    return pd.concat([se_userid_idx, se_cross_userid_post_hour, se_cross_userid_post_weekday,\n",
    "                     se_cross_repost_hour, se_cross_repost_weekday], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca94eea2e714d1b9d05fb8f8388bda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2021-12-09 14:53:40,520 - DEBUG - Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "2021-12-09 14:53:40,522 - DEBUG - Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.704 seconds.\n",
      "2021-12-09 14:53:41,226 - DEBUG - Loading model cost 0.704 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2021-12-09 14:53:41,228 - DEBUG - Prefix dict has been built successfully.\n",
      "2021-12-09 14:53:58,326 - INFO - collecting all words and their counts\n",
      "2021-12-09 14:53:58,327 - INFO - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-12-09 14:53:58,549 - INFO - PROGRESS: at sentence #10000, processed 1151302 words, keeping 80933 word types\n",
      "2021-12-09 14:53:58,779 - INFO - PROGRESS: at sentence #20000, processed 2305735 words, keeping 116286 word types\n",
      "2021-12-09 14:53:58,787 - INFO - collected 117061 word types from a corpus of 2339936 raw words and 20329 sentences\n",
      "2021-12-09 14:53:58,788 - INFO - Creating a fresh vocabulary\n",
      "2021-12-09 14:53:59,226 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 117061 unique words (100.0%% of original 117061, drops 0)', 'datetime': '2021-12-09T14:53:59.225545', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'prepare_vocab'}\n",
      "2021-12-09 14:53:59,227 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 2339936 word corpus (100.0%% of original 2339936, drops 0)', 'datetime': '2021-12-09T14:53:59.227753', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'prepare_vocab'}\n",
      "2021-12-09 14:53:59,907 - INFO - deleting the raw counts dictionary of 117061 items\n",
      "2021-12-09 14:53:59,909 - INFO - sample=0.001 downsamples 31 most-common words\n",
      "2021-12-09 14:53:59,910 - INFO - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1883231.564782688 word corpus (80.5%% of prior 2339936)', 'datetime': '2021-12-09T14:53:59.910547', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'prepare_vocab'}\n",
      "2021-12-09 14:54:01,116 - INFO - estimated required memory for 117061 words and 10 dimensions: 67895380 bytes\n",
      "2021-12-09 14:54:01,117 - INFO - resetting layer weights\n",
      "2021-12-09 14:54:01,123 - INFO - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-12-09T14:54:01.123898', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'build_vocab'}\n",
      "2021-12-09 14:54:01,124 - INFO - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 117061 vocabulary and 10 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2021-12-09T14:54:01.124583', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'train'}\n",
      "2021-12-09 14:54:02,157 - INFO - EPOCH 1 - PROGRESS: at 20.63% examples, 378132 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:03,168 - INFO - EPOCH 1 - PROGRESS: at 43.00% examples, 395147 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:04,202 - INFO - EPOCH 1 - PROGRESS: at 64.35% examples, 398041 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:05,244 - INFO - EPOCH 1 - PROGRESS: at 88.11% examples, 401459 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:05,769 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-09 14:54:05,773 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-09 14:54:05,790 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-09 14:54:05,791 - INFO - EPOCH - 1 : training on 2339936 raw words (1883096 effective words) took 4.7s, 404206 effective words/s\n",
      "2021-12-09 14:54:06,795 - INFO - EPOCH 2 - PROGRESS: at 20.63% examples, 387370 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:07,816 - INFO - EPOCH 2 - PROGRESS: at 42.71% examples, 394519 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:08,818 - INFO - EPOCH 2 - PROGRESS: at 63.61% examples, 399209 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:09,822 - INFO - EPOCH 2 - PROGRESS: at 86.43% examples, 402018 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:10,457 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-09 14:54:10,459 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-09 14:54:10,471 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-09 14:54:10,472 - INFO - EPOCH - 2 : training on 2339936 raw words (1882601 effective words) took 4.7s, 402509 effective words/s\n",
      "2021-12-09 14:54:11,485 - INFO - EPOCH 3 - PROGRESS: at 21.19% examples, 391584 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:12,492 - INFO - EPOCH 3 - PROGRESS: at 43.00% examples, 398888 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:13,530 - INFO - EPOCH 3 - PROGRESS: at 64.35% examples, 400053 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:14,535 - INFO - EPOCH 3 - PROGRESS: at 87.27% examples, 402740 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:15,117 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-09 14:54:15,120 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-09 14:54:15,132 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-09 14:54:15,133 - INFO - EPOCH - 3 : training on 2339936 raw words (1883162 effective words) took 4.7s, 404368 effective words/s\n",
      "2021-12-09 14:54:16,146 - INFO - EPOCH 4 - PROGRESS: at 21.19% examples, 391502 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:17,155 - INFO - EPOCH 4 - PROGRESS: at 43.18% examples, 398910 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:18,194 - INFO - EPOCH 4 - PROGRESS: at 64.35% examples, 399703 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:19,199 - INFO - EPOCH 4 - PROGRESS: at 87.27% examples, 402415 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:19,778 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-09 14:54:19,781 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-09 14:54:19,796 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-09 14:54:19,797 - INFO - EPOCH - 4 : training on 2339936 raw words (1883495 effective words) took 4.7s, 404067 effective words/s\n",
      "2021-12-09 14:54:20,810 - INFO - EPOCH 5 - PROGRESS: at 21.19% examples, 393553 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:21,817 - INFO - EPOCH 5 - PROGRESS: at 43.18% examples, 400261 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:22,856 - INFO - EPOCH 5 - PROGRESS: at 64.35% examples, 400518 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:23,862 - INFO - EPOCH 5 - PROGRESS: at 87.27% examples, 402958 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:54:24,443 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-09 14:54:24,446 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-09 14:54:24,462 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-09 14:54:24,463 - INFO - EPOCH - 5 : training on 2339936 raw words (1883893 effective words) took 4.7s, 404400 effective words/s\n",
      "2021-12-09 14:54:24,464 - INFO - Word2Vec lifecycle event {'msg': 'training on 11699680 raw words (9416247 effective words) took 23.3s, 403458 effective words/s', 'datetime': '2021-12-09T14:54:24.464150', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'train'}\n",
      "2021-12-09 14:54:24,465 - INFO - Word2Vec lifecycle event {'params': 'Word2Vec(vocab=117061, vector_size=10, alpha=0.025)', 'datetime': '2021-12-09T14:54:24.465064', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'created'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c687dce7abc3410ba3376036961debe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 14:54:30,876 - INFO - collecting all words and their counts\n",
      "2021-12-09 14:54:30,878 - INFO - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-12-09 14:54:30,900 - INFO - PROGRESS: at sentence #10000, processed 136451 words, keeping 685 word types\n",
      "2021-12-09 14:54:30,923 - INFO - PROGRESS: at sentence #20000, processed 273297 words, keeping 685 word types\n",
      "2021-12-09 14:54:30,924 - INFO - collected 685 word types from a corpus of 277786 raw words and 20329 sentences\n",
      "2021-12-09 14:54:30,925 - INFO - Creating a fresh vocabulary\n",
      "2021-12-09 14:54:30,929 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 685 unique words (100.0%% of original 685, drops 0)', 'datetime': '2021-12-09T14:54:30.929056', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'prepare_vocab'}\n",
      "2021-12-09 14:54:30,929 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 277786 word corpus (100.0%% of original 277786, drops 0)', 'datetime': '2021-12-09T14:54:30.929922', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'prepare_vocab'}\n",
      "2021-12-09 14:54:30,934 - INFO - deleting the raw counts dictionary of 685 items\n",
      "2021-12-09 14:54:30,935 - INFO - sample=0.001 downsamples 47 most-common words\n",
      "2021-12-09 14:54:30,936 - INFO - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 217059.92209520942 word corpus (78.1%% of prior 277786)', 'datetime': '2021-12-09T14:54:30.936333', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'prepare_vocab'}\n",
      "2021-12-09 14:54:30,944 - INFO - estimated required memory for 685 words and 10 dimensions: 397300 bytes\n",
      "2021-12-09 14:54:30,945 - INFO - resetting layer weights\n",
      "2021-12-09 14:54:30,946 - INFO - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-12-09T14:54:30.946367', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'build_vocab'}\n",
      "2021-12-09 14:54:30,947 - INFO - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 685 vocabulary and 10 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2021-12-09T14:54:30.947051', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'train'}\n",
      "2021-12-09 14:54:31,329 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-09 14:54:31,335 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-09 14:54:31,347 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-09 14:54:31,348 - INFO - EPOCH - 1 : training on 277786 raw words (217162 effective words) took 0.4s, 551498 effective words/s\n",
      "2021-12-09 14:54:31,657 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-09 14:54:31,672 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-09 14:54:31,676 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-09 14:54:31,677 - INFO - EPOCH - 2 : training on 277786 raw words (217258 effective words) took 0.3s, 676415 effective words/s\n",
      "2021-12-09 14:54:31,977 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-09 14:54:31,983 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-09 14:54:31,992 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-09 14:54:31,993 - INFO - EPOCH - 3 : training on 277786 raw words (217214 effective words) took 0.3s, 725991 effective words/s\n",
      "2021-12-09 14:54:32,302 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-09 14:54:32,320 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-09 14:54:32,322 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-09 14:54:32,322 - INFO - EPOCH - 4 : training on 277786 raw words (217116 effective words) took 0.3s, 684481 effective words/s\n",
      "2021-12-09 14:54:32,622 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-09 14:54:32,624 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-09 14:54:32,627 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-09 14:54:32,627 - INFO - EPOCH - 5 : training on 277786 raw words (216854 effective words) took 0.3s, 728384 effective words/s\n",
      "2021-12-09 14:54:32,628 - INFO - Word2Vec lifecycle event {'msg': 'training on 1388930 raw words (1085604 effective words) took 1.7s, 645797 effective words/s', 'datetime': '2021-12-09T14:54:32.628767', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'train'}\n",
      "2021-12-09 14:54:32,629 - INFO - Word2Vec lifecycle event {'params': 'Word2Vec(vocab=685, vector_size=10, alpha=0.025)', 'datetime': '2021-12-09T14:54:32.629772', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'created'}\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:46: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:437: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.505318\n",
      "[200]\ttraining's multi_error: 0.467986\n",
      "[300]\ttraining's multi_error: 0.442151\n",
      "[400]\ttraining's multi_error: 0.413778\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[399]\ttraining's multi_error: 0.413624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.509193\n",
      "[200]\ttraining's multi_error: 0.464961\n",
      "[300]\ttraining's multi_error: 0.439085\n",
      "[400]\ttraining's multi_error: 0.423248\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[395]\ttraining's multi_error: 0.422305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.491088\n",
      "[200]\ttraining's multi_error: 0.461658\n",
      "[300]\ttraining's multi_error: 0.432366\n",
      "[400]\ttraining's multi_error: 0.414556\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\ttraining's multi_error: 0.414556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.497843\n",
      "[200]\ttraining's multi_error: 0.458626\n",
      "[300]\ttraining's multi_error: 0.437972\n",
      "[400]\ttraining's multi_error: 0.418614\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[389]\ttraining's multi_error: 0.41855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_error: 0.500526\n",
      "[200]\ttraining's multi_error: 0.463728\n",
      "[300]\ttraining's multi_error: 0.442794\n",
      "[400]\ttraining's multi_error: 0.417095\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[397]\ttraining's multi_error: 0.416527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cch/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "2021-12-09 14:55:09,795 - INFO - finish text features, cost:89.565840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4754462969810977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cch/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "2021-12-09 14:55:16,873 - INFO - finish weibo create time features, cost:96.643763\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:437: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "2021-12-09 14:55:17,442 - INFO - finish user id target encoding features, cost:97.212162\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d165245b620044d0b30ca5410e779cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbee60dfcd864949bd8feff58d4fe76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a11540652c7458b83c4c86f692405fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/282403 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 14:58:46,157 - INFO - collecting all words and their counts\n",
      "2021-12-09 14:58:46,158 - INFO - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-12-09 14:58:46,181 - INFO - PROGRESS: at sentence #10000, processed 113095 words, keeping 13610 word types\n",
      "2021-12-09 14:58:46,203 - INFO - PROGRESS: at sentence #20000, processed 227764 words, keeping 21528 word types\n",
      "2021-12-09 14:58:46,226 - INFO - PROGRESS: at sentence #30000, processed 344566 words, keeping 27907 word types\n",
      "2021-12-09 14:58:46,247 - INFO - PROGRESS: at sentence #40000, processed 447057 words, keeping 33036 word types\n",
      "2021-12-09 14:58:46,286 - INFO - PROGRESS: at sentence #50000, processed 697288 words, keeping 36417 word types\n",
      "2021-12-09 14:58:46,308 - INFO - PROGRESS: at sentence #60000, processed 799041 words, keeping 40863 word types\n",
      "2021-12-09 14:58:46,343 - INFO - PROGRESS: at sentence #70000, processed 996209 words, keeping 44688 word types\n",
      "2021-12-09 14:58:46,364 - INFO - PROGRESS: at sentence #80000, processed 1099518 words, keeping 48352 word types\n",
      "2021-12-09 14:58:46,388 - INFO - PROGRESS: at sentence #90000, processed 1217429 words, keeping 52141 word types\n",
      "2021-12-09 14:58:46,412 - INFO - PROGRESS: at sentence #100000, processed 1338869 words, keeping 55686 word types\n",
      "2021-12-09 14:58:46,436 - INFO - PROGRESS: at sentence #110000, processed 1460378 words, keeping 58983 word types\n",
      "2021-12-09 14:58:46,460 - INFO - PROGRESS: at sentence #120000, processed 1574870 words, keeping 62301 word types\n",
      "2021-12-09 14:58:46,484 - INFO - PROGRESS: at sentence #130000, processed 1699622 words, keeping 65091 word types\n",
      "2021-12-09 14:58:46,509 - INFO - PROGRESS: at sentence #140000, processed 1821174 words, keeping 68110 word types\n",
      "2021-12-09 14:58:46,536 - INFO - PROGRESS: at sentence #150000, processed 1965229 words, keeping 70802 word types\n",
      "2021-12-09 14:58:46,561 - INFO - PROGRESS: at sentence #160000, processed 2088489 words, keeping 73481 word types\n",
      "2021-12-09 14:58:46,599 - INFO - PROGRESS: at sentence #170000, processed 2225864 words, keeping 76044 word types\n",
      "2021-12-09 14:58:46,631 - INFO - PROGRESS: at sentence #180000, processed 2342058 words, keeping 78461 word types\n",
      "2021-12-09 14:58:46,666 - INFO - PROGRESS: at sentence #190000, processed 2476695 words, keeping 80643 word types\n",
      "2021-12-09 14:58:46,696 - INFO - PROGRESS: at sentence #200000, processed 2581654 words, keeping 83050 word types\n",
      "2021-12-09 14:58:46,731 - INFO - PROGRESS: at sentence #210000, processed 2733640 words, keeping 85201 word types\n",
      "2021-12-09 14:58:46,757 - INFO - PROGRESS: at sentence #220000, processed 2853756 words, keeping 87557 word types\n",
      "2021-12-09 14:58:46,779 - INFO - PROGRESS: at sentence #230000, processed 2953288 words, keeping 89698 word types\n",
      "2021-12-09 14:58:46,824 - INFO - PROGRESS: at sentence #240000, processed 3222401 words, keeping 92045 word types\n",
      "2021-12-09 14:58:46,863 - INFO - PROGRESS: at sentence #250000, processed 3446153 words, keeping 93473 word types\n",
      "2021-12-09 14:58:46,888 - INFO - PROGRESS: at sentence #260000, processed 3567216 words, keeping 95457 word types\n",
      "2021-12-09 14:58:46,912 - INFO - PROGRESS: at sentence #270000, processed 3681918 words, keeping 97664 word types\n",
      "2021-12-09 14:58:46,936 - INFO - PROGRESS: at sentence #280000, processed 3798230 words, keeping 99730 word types\n",
      "2021-12-09 14:58:46,943 - INFO - collected 100297 word types from a corpus of 3828502 raw words and 282403 sentences\n",
      "2021-12-09 14:58:46,944 - INFO - Creating a fresh vocabulary\n",
      "2021-12-09 14:58:47,321 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 100297 unique words (100.0%% of original 100297, drops 0)', 'datetime': '2021-12-09T14:58:47.321008', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'prepare_vocab'}\n",
      "2021-12-09 14:58:47,321 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 3828502 word corpus (100.0%% of original 3828502, drops 0)', 'datetime': '2021-12-09T14:58:47.321857', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'prepare_vocab'}\n",
      "2021-12-09 14:58:47,898 - INFO - deleting the raw counts dictionary of 100297 items\n",
      "2021-12-09 14:58:47,900 - INFO - sample=0.001 downsamples 31 most-common words\n",
      "2021-12-09 14:58:47,901 - INFO - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 2693705.3289155466 word corpus (70.4%% of prior 3828502)', 'datetime': '2021-12-09T14:58:47.901379', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'prepare_vocab'}\n",
      "2021-12-09 14:58:48,925 - INFO - estimated required memory for 100297 words and 10 dimensions: 58172260 bytes\n",
      "2021-12-09 14:58:48,926 - INFO - resetting layer weights\n",
      "2021-12-09 14:58:48,931 - INFO - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-12-09T14:58:48.931972', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'build_vocab'}\n",
      "2021-12-09 14:58:48,932 - INFO - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 100297 vocabulary and 10 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2021-12-09T14:58:48.932515', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'train'}\n",
      "2021-12-09 14:58:49,945 - INFO - EPOCH 1 - PROGRESS: at 17.09% examples, 451824 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:58:50,947 - INFO - EPOCH 1 - PROGRESS: at 35.08% examples, 462229 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:58:51,954 - INFO - EPOCH 1 - PROGRESS: at 54.07% examples, 467198 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:58:52,962 - INFO - EPOCH 1 - PROGRESS: at 73.00% examples, 468355 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:58:53,976 - INFO - EPOCH 1 - PROGRESS: at 86.57% examples, 471540 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:58:54,607 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-09 14:58:54,626 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-09 14:58:54,627 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-09 14:58:54,628 - INFO - EPOCH - 1 : training on 3828502 raw words (2694240 effective words) took 5.7s, 473660 effective words/s\n",
      "2021-12-09 14:58:55,658 - INFO - EPOCH 2 - PROGRESS: at 17.28% examples, 453955 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:58:56,664 - INFO - EPOCH 2 - PROGRESS: at 35.36% examples, 462013 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:58:57,678 - INFO - EPOCH 2 - PROGRESS: at 54.41% examples, 465905 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:58:58,688 - INFO - EPOCH 2 - PROGRESS: at 73.42% examples, 468670 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:58:59,690 - INFO - EPOCH 2 - PROGRESS: at 86.70% examples, 471610 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:59:00,319 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-09 14:59:00,339 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-09 14:59:00,344 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-09 14:59:00,344 - INFO - EPOCH - 2 : training on 3828502 raw words (2694379 effective words) took 5.7s, 472325 effective words/s\n",
      "2021-12-09 14:59:01,358 - INFO - EPOCH 3 - PROGRESS: at 17.28% examples, 460997 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:59:02,366 - INFO - EPOCH 3 - PROGRESS: at 35.90% examples, 472193 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:59:03,376 - INFO - EPOCH 3 - PROGRESS: at 55.01% examples, 473335 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:59:04,387 - INFO - EPOCH 3 - PROGRESS: at 73.62% examples, 474943 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:59:05,390 - INFO - EPOCH 3 - PROGRESS: at 87.72% examples, 477225 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 14:59:05,969 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-09 14:59:05,987 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-09 14:59:05,994 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-09 14:59:05,995 - INFO - EPOCH - 3 : training on 3828502 raw words (2693597 effective words) took 5.6s, 477724 effective words/s\n",
      "2021-12-09 14:59:07,021 - INFO - EPOCH 4 - PROGRESS: at 17.28% examples, 457280 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:59:08,028 - INFO - EPOCH 4 - PROGRESS: at 35.36% examples, 463445 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:59:09,040 - INFO - EPOCH 4 - PROGRESS: at 54.41% examples, 467025 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:59:10,042 - INFO - EPOCH 4 - PROGRESS: at 73.33% examples, 468920 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:59:11,051 - INFO - EPOCH 4 - PROGRESS: at 86.45% examples, 469788 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:59:11,695 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-09 14:59:11,718 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-09 14:59:11,727 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-09 14:59:11,728 - INFO - EPOCH - 4 : training on 3828502 raw words (2693902 effective words) took 5.7s, 471201 effective words/s\n",
      "2021-12-09 14:59:12,746 - INFO - EPOCH 5 - PROGRESS: at 17.28% examples, 457239 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:59:13,752 - INFO - EPOCH 5 - PROGRESS: at 35.59% examples, 467343 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:59:14,756 - INFO - EPOCH 5 - PROGRESS: at 54.67% examples, 470791 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:59:15,767 - INFO - EPOCH 5 - PROGRESS: at 73.48% examples, 471796 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:59:16,794 - INFO - EPOCH 5 - PROGRESS: at 87.00% examples, 471968 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-09 14:59:17,396 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-09 14:59:17,422 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-09 14:59:17,426 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-09 14:59:17,427 - INFO - EPOCH - 5 : training on 3828502 raw words (2693458 effective words) took 5.7s, 473293 effective words/s\n",
      "2021-12-09 14:59:17,427 - INFO - Word2Vec lifecycle event {'msg': 'training on 19142510 raw words (13469576 effective words) took 28.5s, 472703 effective words/s', 'datetime': '2021-12-09T14:59:17.427898', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'train'}\n",
      "2021-12-09 14:59:17,429 - INFO - Word2Vec lifecycle event {'params': 'Word2Vec(vocab=100297, vector_size=10, alpha=0.025)', 'datetime': '2021-12-09T14:59:17.429098', 'gensim': '4.1.2', 'python': '3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-45-generic-x86_64-with-debian-stretch-sid', 'event': 'created'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7afac7acd364060a7848b56ac64f902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cch/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  \n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:54: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "2021-12-09 15:18:19,114 - INFO - finish repost features, cost:1478.884817\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_weibo = pd.merge(df_origin_weibo, df_user_profile_agg, how='left', on='UserId')\n",
    "\n",
    "#文本特征\n",
    "df_weibo['intro'] = df_weibo['intro'].fillna(' ')\n",
    "\n",
    "weibotext_wv_embed = generate_wv(df_weibo['WeiboText']) \n",
    "df_weibo['weibotext_wv_embed'] = list(weibotext_wv_embed) #微博内容词向量\n",
    "\n",
    "user_intro_wv_embed = generate_wv(df_weibo['intro'])\n",
    "df_weibo['user_intro_wv_embed'] = list(user_intro_wv_embed) #用户简介词向量\n",
    "\n",
    "weibotext_tfidf = build_tfidf_svd_matrix(df_weibo['WeiboText'], 10)\n",
    "df_weibo['weibotext_tfidf'] = list(weibotext_tfidf) #微博内容tfidf+svd\n",
    "\n",
    "user_intro_tfidf = build_tfidf_svd_matrix(df_weibo['intro'], 10)\n",
    "df_weibo['user_intro_tfidf'] = list(user_intro_tfidf)#用户简介tfidf+svd\n",
    "df_weibo = pd.concat([df_weibo,get_text_hard_feature()],1)\n",
    "\n",
    "df_text_stacking = get_text_stacking_feature(df_weibo)\n",
    "df_weibo = pd.concat([df_weibo, df_text_stacking], 1)\n",
    "logging.info('finish text features, cost:%f'%(time.time()-start))\n",
    "\n",
    "#微博创建时间特征\n",
    "df_weibo_create_time_feature = df_weibo['WeiboCreateTime'].apply(apply_weibo_creatime)\n",
    "df_weibo = pd.concat([df_weibo, df_weibo_create_time_feature], 1)\n",
    "logging.info('finish weibo create time features, cost:%f'%(time.time()-start))\n",
    "\n",
    "#微博用户target encoding\n",
    "df_weibo = set_userid_target_encode(df_weibo)\n",
    "logging.info('finish user id target encoding features, cost:%f'%(time.time()-start))\n",
    "\n",
    "#转发特征\n",
    "mp_WeiboId_create_time = dict(list(df_weibo[['WeiboId', 'WeiboCreateTime']].values))\n",
    "df_train_repost['pass_time'] = df_train_repost.apply(apply_pass_time, 1)\n",
    "df_test_repost['pass_time'] = df_test_repost.apply(apply_pass_time, 1)\n",
    "\n",
    "df_train_repost_1h = df_train_repost.query('pass_time < 3600')\n",
    "df_repost_1h = pd.concat([df_train_repost_1h, df_test_repost])\n",
    "\n",
    "df_repost_weibo_1h_agg = generate_1h_features(df_user_profile_agg, df_repost_1h, 'weibo') #1h内微博维度特征\n",
    "df_repost_userid_1h_agg = generate_1h_features(df_user_profile_agg, df_repost_1h, 'userid') #1h内用户维度特征\n",
    "\n",
    "df_weibo = pd.merge(df_weibo, df_repost_weibo_1h_agg, how='left', on='WeiboId')\n",
    "df_weibo = df_weibo.fillna(-1)\n",
    "df_weibo = pd.merge(df_weibo, df_repost_userid_1h_agg, how='left', on='UserId')\n",
    "\n",
    "#转发特征聚合\n",
    "df_repost_user_set_1h = get_repost_user_set(df_repost_1h, df_user_profile_agg, set(df_weibo['WeiboId']))\n",
    "df_weibo = pd.merge(df_weibo, df_repost_user_set_1h, how='left', on='WeiboId')\n",
    "\n",
    "# 交叉特征\n",
    "df_cross = get_cross_feature()\n",
    "df_weibo = pd.concat([df_weibo, df_cross], 1)\n",
    "\n",
    "logging.info('finish repost features, cost:%f'%(time.time()-start))\n",
    "\n",
    "pickle.dump(df_weibo, open('%s/df_all.pickle'%VAR_PATH, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 补充交叉特征注入24H转发"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:437: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "def get_24h_cross_target_label(df_weibo, df_labels):\n",
    "    \n",
    "    df_weibo_copy = copy.deepcopy(df_weibo)\n",
    "    df_labels_copy = copy.deepcopy(df_labels)\n",
    "    target_name_list = sorted(list(set(df_labels_copy.columns) - set(['WeiboId'])))\n",
    "    n_label = len(target_name_list)\n",
    "    df_weibo_copy = pd.merge(df_weibo_copy, df_labels_copy, how='left', on='WeiboId')\n",
    "        \n",
    "    n_flod = 5\n",
    "    folds = KFold(n_splits=n_flod, shuffle=True)\n",
    "    train_x = np.array(df_weibo_copy.iloc[:18000]['cross_userid_idx_post_hour']).astype('str')\n",
    "    train_labels = np.array(df_weibo_copy[target_name_list].iloc[:18000].values).astype('float64')\n",
    "    test_x = np.array(df_weibo_copy.iloc[18000:]['cross_userid_idx_post_hour']).astype('str')\n",
    "    \n",
    "    target_encode_train = np.zeros((len(train_x), n_label))\n",
    "    target_encode_test = np.zeros((len(test_x), n_label))\n",
    "    \n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_x, train_labels)):\n",
    "        \n",
    "        trn_x, trn_labels = train_x[trn_idx], train_labels[trn_idx]\n",
    "        val_x, val_labels = train_x[val_idx], train_labels[val_idx]\n",
    "        \n",
    "        \n",
    "        for i in range(n_label):\n",
    "            encoder = TargetEncoder(smoothing=0)\n",
    "#             print(trn_x, trn_labels[:, i])\n",
    "            encoder.fit(trn_x, trn_labels[:, i])\n",
    "            target_encode_train[val_idx, i:i+1] = encoder.transform(val_x)\n",
    "#             print(encoder.transform(trn_x).values)\n",
    "#             print(trn_x)\n",
    "#             print(trn_labels[:, i])\n",
    "#             print('*'*10)\n",
    "            target_encode_test[:, i:i+1] += encoder.transform(test_x)/n_flod\n",
    "    \n",
    "    column_names = ['target_encode_%s'%s for s in target_name_list]\n",
    "    \n",
    "    df_ret = pd.DataFrame()\n",
    "    df_ret.index = df_weibo_copy.index\n",
    "    df_ret['WeiboId'] = df_weibo['WeiboId']\n",
    "    target_encode_score = np.concatenate([target_encode_train, target_encode_test])\n",
    "    for i in range(len(column_names)):\n",
    "        df_ret[column_names[i]] = list(target_encode_score[:, i])\n",
    "    \n",
    "    return df_ret\n",
    "    \n",
    "    \n",
    "cross_target_encode_name = ['target_encode_24h_10_reposet_cnt',\n",
    "       'target_encode_24h_11_reposet_cnt', 'target_encode_24h_12_reposet_cnt',\n",
    "       'target_encode_24h_13_reposet_cnt', 'target_encode_24h_14_reposet_cnt',\n",
    "       'target_encode_24h_15_reposet_cnt', 'target_encode_24h_16_reposet_cnt',\n",
    "       'target_encode_24h_17_reposet_cnt', 'target_encode_24h_18_reposet_cnt',\n",
    "       'target_encode_24h_19_reposet_cnt', 'target_encode_24h_20_reposet_cnt',\n",
    "       'target_encode_24h_21_reposet_cnt', 'target_encode_24h_22_reposet_cnt',\n",
    "       'target_encode_24h_23_reposet_cnt', 'target_encode_24h_24_reposet_cnt',\n",
    "       'target_encode_24h_2_reposet_cnt', 'target_encode_24h_3_reposet_cnt',\n",
    "       'target_encode_24h_4_reposet_cnt', 'target_encode_24h_5_reposet_cnt',\n",
    "       'target_encode_24h_6_reposet_cnt', 'target_encode_24h_7_reposet_cnt',\n",
    "       'target_encode_24h_8_reposet_cnt', 'target_encode_24h_9_reposet_cnt']\n",
    "\n",
    "df_repost_24h_origin = pickle.load(open('%s/df_repost_24h_origin.pickle'%VAR_PATH, 'rb'))\n",
    "df_cross_feature_target_encode_24h = get_24h_cross_target_label(df_weibo, df_repost_24h_origin)\n",
    "df_weibo = pd.merge(df_weibo, df_cross_feature_target_encode_24h, how='left', on='WeiboId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df_weibo, open('%s/df_all.pickle'%VAR_PATH, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyr",
   "language": "python",
   "name": "hyr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "290.455px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
