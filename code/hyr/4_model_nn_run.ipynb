{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cch/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.8' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/dask/config.py:161: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/dask/dataframe/utils.py:13: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "from gensim.models.word2vec import Word2Vec \n",
    "import jieba\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "import IPython.display as ipd\n",
    "from collections import *\n",
    "import torch\n",
    "from transformers import *\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "LOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)\n",
    "from sklearn.utils.extmath import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_PATH = '/data/cch/hyr/weibo/var'\n",
    "df_weibo = pickle.load(open('%s/df_all.pickle'%VAR_PATH, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/data/cch/weibo'\n",
    "df_user_profile = pd.read_csv('%s/user_profile.csv'%DATA_PATH, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_24h_feature_predict = pickle.load(open('%s/df_24h_feature_predict.pickle'%VAR_PATH, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_set = set(df_weibo['UserId'])\n",
    "# df_user_profile[df_user_profile['UserId'].apply(lambda x : x in user_set)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "feature_24h_list = list(df_24h_feature_predict.columns)\n",
    "\n",
    "continue_feature_list = [\n",
    "\n",
    "    #文本硬特征\n",
    "    'weibotext_len', \n",
    "    \n",
    "    #性别\n",
    "    'Gender', \n",
    "    \n",
    "    #用户粉丝，关注数\n",
    "    'follow_mean', 'follow_median', 'follow_max', 'follow_min', 'follower_mean',\n",
    "    'follower_median', 'follower_max', 'follower_min',\n",
    "    \n",
    "    #user id target encode\n",
    "#     'target_encode_0', 'target_encode_1', 'target_encode_2', 'target_encode_3', 'target_encode_4',\n",
    "#     'target_encode_cnt', \n",
    "    \n",
    "    #转发特征(微博维度)\n",
    "     'repost_weibo_cnt_15_30_mins', 'repost_weibo_cnt_15_mins','repost_weibo_cnt_30_45_mins','repost_weibo_cnt_30_mins',\n",
    "     'repost_weibo_cnt_45_60_mins','repost_weibo_cnt_45_mins','repost_weibo_cnt_60_mins',\n",
    "     'repost_weibo_follow_max_max',\n",
    "     'repost_weibo_follow_max_mean','repost_weibo_follow_max_min','repost_weibo_follow_max_sum','repost_weibo_follow_mean_max',\n",
    "     'repost_weibo_follow_mean_mean','repost_weibo_follow_mean_min','repost_weibo_follow_mean_sum','repost_weibo_follow_median_max',\n",
    "     'repost_weibo_follow_median_mean','repost_weibo_follow_median_min','repost_weibo_follow_median_sum',\n",
    "     'repost_weibo_follow_min_max', 'repost_weibo_follow_min_mean','repost_weibo_follow_min_min','repost_weibo_follow_min_sum',\n",
    "     'repost_weibo_follower_max_max','repost_weibo_follower_max_mean','repost_weibo_follower_max_min',\n",
    "     'repost_weibo_follower_max_sum','repost_weibo_follower_mean_max','repost_weibo_follower_mean_mean',\n",
    "     'repost_weibo_follower_mean_min','repost_weibo_follower_mean_sum','repost_weibo_follower_median_max',\n",
    "     'repost_weibo_follower_median_mean','repost_weibo_follower_median_min','repost_weibo_follower_median_sum',\n",
    "     'repost_weibo_follower_min_max','repost_weibo_follower_min_mean','repost_weibo_follower_min_min',\n",
    "     'repost_weibo_follower_min_sum','repost_weibo_pass_time_max','repost_weibo_pass_time_mean','repost_weibo_pass_time_median',\n",
    "     'repost_weibo_pass_time_min', 'repost_weibo_Verified', 'repost_weibo_Gender',\n",
    "    \n",
    "    #水军\n",
    "     'repost_weibo_unique_repost_userid',\n",
    "     'repost_weibo_repeat_repost_cnt',\n",
    "     'repost_weibo_max_user_repost',\n",
    "     'repost_weibo_other_cnt_max',\n",
    "     'repost_weibo_other_cnt_min',\n",
    "     'repost_weibo_other_cnt_mean',\n",
    "     'repost_weibo_other_cnt_std',\n",
    "     'repost_weibo_other_cnt_0',\n",
    "     'repost_weibo_text_len_max',\n",
    "     'repost_weibo_text_len_min',\n",
    "     'repost_weibo_text_len_mean',\n",
    "     'repost_weibo_text_len_std',\n",
    "\n",
    "    #转发特征(用户维度)    \n",
    "     'repost_userid_cnt_15_30_mins','repost_userid_cnt_15_mins','repost_userid_cnt_30_45_mins','repost_userid_cnt_30_mins',\n",
    "     'repost_userid_cnt_45_60_mins','repost_userid_cnt_45_mins','repost_userid_cnt_60_mins','repost_userid_follow_max_max',\n",
    "     'repost_userid_follow_max_mean','repost_userid_follow_max_min','repost_userid_follow_max_sum','repost_userid_follow_mean_max',\n",
    "     'repost_userid_follow_mean_mean','repost_userid_follow_mean_min','repost_userid_follow_mean_sum',\n",
    "     'repost_userid_follow_median_max','repost_userid_follow_median_mean','repost_userid_follow_median_min',\n",
    "     'repost_userid_follow_median_sum','repost_userid_follow_min_max','repost_userid_follow_min_mean',\n",
    "     'repost_userid_follow_min_min','repost_userid_follow_min_sum','repost_userid_follower_max_max',\n",
    "     'repost_userid_follower_max_mean','repost_userid_follower_max_min','repost_userid_follower_max_sum',\n",
    "     'repost_userid_follower_mean_max','repost_userid_follower_mean_mean','repost_userid_follower_mean_min',\n",
    "     'repost_userid_follower_mean_sum','repost_userid_follower_median_max','repost_userid_follower_median_mean',\n",
    "     'repost_userid_follower_median_min','repost_userid_follower_median_sum','repost_userid_follower_min_max',\n",
    "     'repost_userid_follower_min_mean','repost_userid_follower_min_min','repost_userid_follower_min_sum',\n",
    "     'repost_userid_pass_time_max','repost_userid_pass_time_mean','repost_userid_pass_time_median',\n",
    "     'repost_userid_pass_time_min', 'repost_userid_Verified', 'repost_userid_Gender',\n",
    "        \n",
    "     'repost_userid_unique_repost_userid',\n",
    "     'repost_userid_repeat_repost_cnt',\n",
    "     'repost_userid_max_user_repost',\n",
    "     'repost_userid_other_cnt_max',\n",
    "     'repost_userid_other_cnt_mean',\n",
    "     'repost_userid_other_cnt_std',\n",
    "     'repost_userid_other_cnt_0',\n",
    "     'repost_userid_text_len_max',\n",
    "     'repost_userid_text_len_min',\n",
    "     'repost_userid_text_len_mean',\n",
    "     'repost_userid_text_len_std',\n",
    "    \n",
    "    \n",
    "    #交叉特征 24h转发特征 target encode\n",
    "#     'target_encode_24h_10_reposet_cnt',\n",
    "#        'target_encode_24h_11_reposet_cnt', 'target_encode_24h_12_reposet_cnt',\n",
    "#        'target_encode_24h_13_reposet_cnt', 'target_encode_24h_14_reposet_cnt',\n",
    "#        'target_encode_24h_15_reposet_cnt', 'target_encode_24h_16_reposet_cnt',\n",
    "#        'target_encode_24h_17_reposet_cnt', 'target_encode_24h_18_reposet_cnt',\n",
    "#        'target_encode_24h_19_reposet_cnt', 'target_encode_24h_20_reposet_cnt',\n",
    "#        'target_encode_24h_21_reposet_cnt', 'target_encode_24h_22_reposet_cnt',\n",
    "#        'target_encode_24h_23_reposet_cnt', 'target_encode_24h_24_reposet_cnt',\n",
    "#        'target_encode_24h_2_reposet_cnt', 'target_encode_24h_3_reposet_cnt',\n",
    "#        'target_encode_24h_4_reposet_cnt', 'target_encode_24h_5_reposet_cnt',\n",
    "#        'target_encode_24h_6_reposet_cnt', 'target_encode_24h_7_reposet_cnt',\n",
    "#        'target_encode_24h_8_reposet_cnt', 'target_encode_24h_9_reposet_cnt'\n",
    "]\n",
    "\n",
    "cat_feature_list = [\n",
    "    #\n",
    "    '转发&点赞', '疫情', '特朗普|总统', '视频|链接', '粉|饭', '中国&金牌', '台湾',\n",
    "    '历史', '发展|经济', '推荐', '东京&奥运',    \n",
    "\n",
    "    #发表日期特征\n",
    "    'post_day', 'post_weekday', 'post_month', 'post_hour',\n",
    "    \n",
    "    #交叉特征\n",
    "    'userid_idx', \n",
    "    'cross_userid_idx_post_hour',\n",
    "    'cross_userid_idx_post_weekday', \n",
    "    'cross_repost_1h_cnt_idx_post_hour', 'cross_repost_1h_cnt_idx_post_weekday',\n",
    "]\n",
    "\n",
    "hidden_feature_list = [\n",
    "    'weibotext_wv_embed', 'weibotext_tfidf',#微博内容词向量/tfidf+svd降维\n",
    "    'user_intro_wv_embed', 'user_intro_tfidf'#用户个性签名词向量/tfidf+svd降维\n",
    "]\n",
    "\n",
    "\n",
    "def generate_one_hot(df_weibo):\n",
    "    global cat_feature_list\n",
    "    \n",
    "    cut_one_hot_feature_list = [    \n",
    "        #转发特征(微博维度)\n",
    "         'repost_weibo_cnt_15_30_mins', 'repost_weibo_cnt_15_mins','repost_weibo_cnt_30_45_mins','repost_weibo_cnt_30_mins',\n",
    "         'repost_weibo_cnt_45_60_mins','repost_weibo_cnt_45_mins','repost_weibo_cnt_60_mins','repost_weibo_follow_max_max',\n",
    "         'repost_weibo_follow_max_mean','repost_weibo_follow_max_min','repost_weibo_follow_max_sum','repost_weibo_follow_mean_max',\n",
    "         'repost_weibo_follow_mean_mean','repost_weibo_follow_mean_min','repost_weibo_follow_mean_sum','repost_weibo_follow_median_max',\n",
    "         'repost_weibo_follow_median_mean','repost_weibo_follow_median_min','repost_weibo_follow_median_sum',\n",
    "         'repost_weibo_follow_min_max', 'repost_weibo_follow_min_mean','repost_weibo_follow_min_min','repost_weibo_follow_min_sum',\n",
    "         'repost_weibo_follower_max_max','repost_weibo_follower_max_mean','repost_weibo_follower_max_min',\n",
    "         'repost_weibo_follower_max_sum','repost_weibo_follower_mean_max','repost_weibo_follower_mean_mean',\n",
    "         'repost_weibo_follower_mean_min','repost_weibo_follower_mean_sum','repost_weibo_follower_median_max',\n",
    "         'repost_weibo_follower_median_mean','repost_weibo_follower_median_min','repost_weibo_follower_median_sum',\n",
    "         'repost_weibo_follower_min_max','repost_weibo_follower_min_mean','repost_weibo_follower_min_min',\n",
    "         'repost_weibo_follower_min_sum','repost_weibo_pass_time_max','repost_weibo_pass_time_mean','repost_weibo_pass_time_median',\n",
    "         'repost_weibo_pass_time_min', 'repost_weibo_Verified', 'repost_weibo_Gender',\n",
    "\n",
    "         'repost_weibo_unique_repost_userid',\n",
    "         'repost_weibo_repeat_repost_cnt',\n",
    "         'repost_weibo_max_user_repost',\n",
    "         'repost_weibo_other_cnt_max',\n",
    "         'repost_weibo_other_cnt_min',\n",
    "         'repost_weibo_other_cnt_mean',\n",
    "         'repost_weibo_other_cnt_std',\n",
    "         'repost_weibo_other_cnt_0',\n",
    "         'repost_weibo_text_len_max',\n",
    "         'repost_weibo_text_len_min',\n",
    "         'repost_weibo_text_len_mean',\n",
    "         'repost_weibo_text_len_std',\n",
    "\n",
    "        #转发特征(用户维度)    \n",
    "         'repost_userid_cnt_15_30_mins','repost_userid_cnt_15_mins','repost_userid_cnt_30_45_mins','repost_userid_cnt_30_mins',\n",
    "         'repost_userid_cnt_45_60_mins','repost_userid_cnt_45_mins','repost_userid_cnt_60_mins','repost_userid_follow_max_max',\n",
    "         'repost_userid_follow_max_mean','repost_userid_follow_max_min','repost_userid_follow_max_sum','repost_userid_follow_mean_max',\n",
    "         'repost_userid_follow_mean_mean','repost_userid_follow_mean_min','repost_userid_follow_mean_sum',\n",
    "         'repost_userid_follow_median_max','repost_userid_follow_median_mean','repost_userid_follow_median_min',\n",
    "         'repost_userid_follow_median_sum','repost_userid_follow_min_max','repost_userid_follow_min_mean',\n",
    "         'repost_userid_follow_min_min','repost_userid_follow_min_sum','repost_userid_follower_max_max',\n",
    "         'repost_userid_follower_max_mean','repost_userid_follower_max_min','repost_userid_follower_max_sum',\n",
    "         'repost_userid_follower_mean_max','repost_userid_follower_mean_mean','repost_userid_follower_mean_min',\n",
    "         'repost_userid_follower_mean_sum','repost_userid_follower_median_max','repost_userid_follower_median_mean',\n",
    "         'repost_userid_follower_median_min','repost_userid_follower_median_sum','repost_userid_follower_min_max',\n",
    "         'repost_userid_follower_min_mean','repost_userid_follower_min_min','repost_userid_follower_min_sum',\n",
    "         'repost_userid_pass_time_max','repost_userid_pass_time_mean','repost_userid_pass_time_median',\n",
    "         'repost_userid_pass_time_min', 'repost_userid_Verified', 'repost_userid_Gender',\n",
    "\n",
    "         'repost_userid_unique_repost_userid',\n",
    "         'repost_userid_repeat_repost_cnt',\n",
    "         'repost_userid_max_user_repost',\n",
    "         'repost_userid_other_cnt_max',\n",
    "    #      'repost_userid_other_cnt_min',\n",
    "         'repost_userid_other_cnt_mean',\n",
    "         'repost_userid_other_cnt_std',\n",
    "         'repost_userid_other_cnt_0',\n",
    "         'repost_userid_text_len_max',\n",
    "         'repost_userid_text_len_min',\n",
    "         'repost_userid_text_len_mean',\n",
    "         'repost_userid_text_len_std',\n",
    "    ]\n",
    "\n",
    "    one_hot_feature_name_list = []\n",
    "    df_one_hot_list = []\n",
    "    for f in cat_feature_list:\n",
    "        df_one_hot = pd.get_dummies(df_weibo[f])\n",
    "        one_hot_feature_name = ['%s_%s'%(f, name) for name in df_one_hot.columns]\n",
    "        one_hot_feature_name_list.extend(one_hot_feature_name)\n",
    "        df_one_hot.columns = one_hot_feature_name \n",
    "        df_one_hot_list.append(df_one_hot)\n",
    "\n",
    "    cut_num = 5\n",
    "    for f in cut_one_hot_feature_list:\n",
    "        df_one_hot = pd.get_dummies(pd.qcut(df_weibo[f], cut_num, duplicates=\"drop\"))\n",
    "        one_hot_feature_name = ['%s_qcut_%s'%(f, name) for name in df_one_hot.columns]\n",
    "        one_hot_feature_name_list.extend(one_hot_feature_name)\n",
    "        df_one_hot.columns = one_hot_feature_name \n",
    "        df_one_hot_list.append(df_one_hot)\n",
    "\n",
    "        df_one_hot = pd.get_dummies(pd.cut(df_weibo[f], cut_num))\n",
    "        one_hot_feature_name = ['%s_cut_%s'%(f, name) for name in df_one_hot.columns]\n",
    "        one_hot_feature_name_list.extend(one_hot_feature_name)\n",
    "        df_one_hot.columns = one_hot_feature_name \n",
    "        df_one_hot_list.append(df_one_hot)\n",
    "    return pd.concat(df_one_hot_list, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cch/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:198: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_weibo.index = df_weibo.WeiboId\n",
    "\n",
    "# df_weibo['sample_weight'] = df_weibo['label'].map({0:1,1:10,2:50,3:100,4:300,-1:0})\n",
    "df_weibo['sample_weight'] = df_weibo['label'].map({0:0.00333333,1:0.03333333,2:0.16666667,3:0.33333333,4:1,-1:0})\n",
    "df_weibo['label_10'] = df_weibo['ForwordCount'].apply(lambda x : int(x//10) if x < 300 else 30).value_counts()\n",
    "\n",
    "df_weibo_one_hot = generate_one_hot(df_weibo)\n",
    "one_hot_feature_list = list(df_weibo_one_hot.columns)\n",
    "df_weibo = pd.concat([df_weibo, df_weibo_one_hot], 1)\n",
    "df_weibo = pd.concat([df_weibo, df_24h_feature_predict], 1)\n",
    "\n",
    "\n",
    "\n",
    "dense_mean = np.array(df_weibo[continue_feature_list].mean()) \n",
    "dense_std = np.array(df_weibo[continue_feature_list].std())\n",
    "\n",
    "forword_cnt_mean = df_weibo.query('type==\"train\"')['ForwordCount'].mean()\n",
    "forword_cnt_std = df_weibo.query('type==\"train\"')['ForwordCount'].std()\n",
    "\n",
    "mp_userid_idx = dict([(userid,i) for i, userid in enumerate(set(df_weibo['UserId']))])\n",
    "\n",
    "# train_x = np.array(df_weibo.query('type==\"train\"')[continue_feature_list]).astype('float32')\n",
    "# train_label = np.array(df_weibo.query('type==\"train\"')['label']).astype('int')\n",
    "# train_forward_cnt = np.array(df_weibo.query('type==\"train\"')['ForwordCount']).astype('float32')\n",
    "\n",
    "# test_x = np.array(df_weibo.query('type==\"test\"')[continue_feature_list]).astype('float32')\n",
    "\n",
    "# train_x, test_x = (train_x - mean)/std, (test_x - mean)/std\n",
    "\n",
    "# train_x.shape, train_y.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distibution(cnt):\n",
    "    ret = np.zeros(5)\n",
    "    if cnt > 400:\n",
    "        cnt = 400\n",
    "    ll = [-10,11,51,151,301]\n",
    "    rr = [10,50,150,300,500]\n",
    "    median = [(l+r)//2 for l,r in zip(ll, rr)]\n",
    "    interval_length = [r-l+3 for l, r in zip(ll, rr)]\n",
    "    eps = 1e-8\n",
    "    for i in range(5):\n",
    "        dis = abs(cnt - median[i])/interval_length[i]\n",
    "        ret[i] = 1/(dis+eps)\n",
    "    return ret/sum(ret)\n",
    "\n",
    "def get_distibution_softmax(cnt):\n",
    "    ret = np.zeros(5)\n",
    "    if cnt > 400:\n",
    "        cnt = 400\n",
    "    ll = [-10,11,51,151,301]\n",
    "    rr = [10,50,150,300,500]\n",
    "    median = [(l+r)//2 for l,r in zip(ll, rr)]\n",
    "    interval_length = [r-l+3 for l, r in zip(ll, rr)]\n",
    "    eps = 1e-8\n",
    "    for i in range(5):\n",
    "        dis = abs(cnt - median[i])/interval_length[i]\n",
    "        ret[i] = dis\n",
    "    return softmax([np.array(-ret)])[0]\n",
    "\n",
    "def get_mp_distibution(func_name):\n",
    "    ret = {}\n",
    "    for i in range(500):\n",
    "        ret[i] = func_name(i)\n",
    "    return ret\n",
    "\n",
    "mp_distibution = get_mp_distibution(get_distibution)\n",
    "mp_distibution_softmax = get_mp_distibution(get_distibution_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class WeiboDataset(Data.Dataset):\n",
    "    def __init__(self, weibo_id):\n",
    "        self.weibo_id = weibo_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.weibo_id)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.weibo_id[idx]\n",
    "\n",
    "def collate_fn(weibo_ids, cal_loss=True):\n",
    "\n",
    "    global dense_mean, dense_std, df_weibo, albert_tokenizer, mp_userid_idx, one_hot_feature_list, mp_distibution\n",
    "        \n",
    "    df_weibo_sub = df_weibo.loc[weibo_ids]\n",
    "    #连续性特征归一化\n",
    "    dense = (np.array(df_weibo_sub[continue_feature_list].values) - dense_mean) /dense_std \n",
    "    soft_label = \\\n",
    "    np.array(list(df_weibo_sub['ForwordCount'].apply(lambda x : mp_distibution.get(x,-1) if x < 400 else mp_distibution[400])))\n",
    "    \n",
    "    #标签\n",
    "    y = np.array(df_weibo_sub['label'])\n",
    "    forword_cnt = np.array(df_weibo_sub['ForwordCount'])\n",
    "    #样本权重\n",
    "    sample_weight = np.array(df_weibo_sub['sample_weight'])\n",
    "    \n",
    "    #序列文本特征用于bert encoder\n",
    "    #微博正文\n",
    "#     df_weibo_text = df_weibo_sub['WeiboText']\n",
    "#     max_length = min(args.weibo_text_max_length, max(df_weibo_text.apply(len)))\n",
    "#     weibotext_tokenized_list = []\n",
    "#     for sentence in df_weibo_text:\n",
    "#         sentence = list(sentence)\n",
    "#         if len(sentence) > max_length:\n",
    "#             text = sentence[:max_length]\n",
    "#         else:\n",
    "#             text = sentence + ['<pad>']*(max_length-len(sentence))\n",
    "#         weibotext_tokenized_list.append(albert_tokenizer.encode(text))        \n",
    "    \n",
    "    #用户简介\n",
    "#     max_length = max(df_weibo_sub['intro'].apply(len))\n",
    "#     intro_tokenized_list = []\n",
    "#     for sentence in df_weibo_sub['intro']:\n",
    "#         sentence = list(sentence)\n",
    "#         if len(sentence) > max_length:\n",
    "#             text = sentence[:max_length]\n",
    "#         else:\n",
    "#             text = sentence + ['<pad>']*(max_length-len(sentence))\n",
    "#         intro_tokenized_list.append(albert_tokenizer.encode(text))        \n",
    "    \n",
    "    #固定文本特征\n",
    "    text_feature_list = []\n",
    "    for f in ['weibotext_wv_embed', 'weibotext_tfidf', 'user_intro_wv_embed', 'user_intro_tfidf']:\n",
    "        hidden_train = np.array(list(df_weibo_sub[f]))\n",
    "        text_feature_list.append(hidden_train)\n",
    "    text_hidden = np.concatenate(text_feature_list, 1)\n",
    "    \n",
    "    user_id_token = np.array(df_weibo_sub['UserId'].map(mp_userid_idx))\n",
    "    \n",
    "    #转发用户+转发文本 聚合\n",
    "    max_repost_user_set = 512\n",
    "    np_repost_text_wv = np.zeros((len(weibo_ids), max_repost_user_set, 10))\n",
    "    np_repost_user_dense = np.zeros((len(weibo_ids), max_repost_user_set, 8))\n",
    "    np_repost_user_id = np.zeros((len(weibo_ids), max_repost_user_set)).astype('int')\n",
    "    np_repost_set_len = np.zeros((len(weibo_ids),))\n",
    "    for i, weiboid in enumerate(weibo_ids):\n",
    "        userid_set_len = min(max_repost_user_set, df_weibo_sub.at[weiboid, 'repost_weibo_set_len'])\n",
    "        np_repost_text_wv[i,:userid_set_len,:] = np.array(df_weibo_sub.at[weiboid, 'repost_weibo_repost_text_wv'])[:userid_set_len]\n",
    "        np_repost_user_id[i,:userid_set_len] = np.array(df_weibo_sub.at[weiboid, 'repost_weibo_repost_user_id_freq'])[:userid_set_len]\n",
    "        np_repost_set_len[i] = userid_set_len\n",
    "        np_repost_user_dense[i,:userid_set_len] = np.array(df_weibo_sub.at[weiboid, 'repost_weibo_set_user_dense'])[:userid_set_len]\n",
    "    \n",
    "    return {\n",
    "        'cal_loss':cal_loss,\n",
    "        'x_dense':torch.tensor(dense).float().to(args.device), \n",
    "        'label':torch.tensor(y).long().to(args.device),\n",
    "        'label_10':torch.tensor(np.array(df_weibo_sub['label_10'])).long().to(args.device),\n",
    "        'forword_cnt' : torch.tensor(forword_cnt).float().to(args.device),\n",
    "        'sample_weight':torch.tensor(sample_weight).float().to(args.device),\n",
    "#         'weibo_text_token':torch.tensor(weibotext_tokenized_list).to(args.device),\n",
    "#         'intro_text_token':torch.tensor(intro_tokenized_list).to(args.device),\n",
    "        'text_wv_tfidf':torch.tensor(text_hidden).float().to(args.device),\n",
    "        'user_id_token':torch.tensor(user_id_token).long().to(args.device),\n",
    "        \n",
    "        'repost_set_text_wv':torch.tensor(np_repost_text_wv).float().to(args.device),\n",
    "        'repost_set_userid':torch.tensor(np_repost_user_id).long().to(args.device),\n",
    "        'repost_set_len' : torch.tensor(np_repost_set_len).long().to(args.device),\n",
    "        'repost_user_dense':torch.tensor(np_repost_user_dense).float().to(args.device),\n",
    "        'one_hot_feature':torch.tensor(np.array(df_weibo_sub[one_hot_feature_list])).float().to(args.device),\n",
    "        'soft_label':torch.tensor(soft_label).float().to(args.device),\n",
    "    }\n",
    "\n",
    "# train_dataset=WeiboDataset(train_weibo_id_list)\n",
    "# data_loader = Data.DataLoader(\n",
    "#     dataset=train_dataset,      \n",
    "#     batch_size=args.batch_size,      \n",
    "#     shuffle=True,\n",
    "#     collate_fn=collate_fn,\n",
    "#     num_workers = args.n_worker,\n",
    "# )\n",
    "# for _ in data_loader:\n",
    "#     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     1,
     5,
     54,
     75
    ]
   },
   "outputs": [],
   "source": [
    "class GeLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1. + torch.tanh(x * 0.7978845608 * (1. + 0.044715 * x * x)))\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, activations=False):\n",
    "        super().__init__()\n",
    "        linear = nn.Linear(in_features, out_features)\n",
    "        nn.init.normal_(linear.weight, std=math.sqrt((2. if activations else 1.) / in_features))\n",
    "        nn.init.zeros_(linear.bias)\n",
    "        modules = [nn.utils.weight_norm(linear)]\n",
    "        if activations:\n",
    "            modules.append(GeLU())\n",
    "        self.model = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class CeLossOut(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.out_label_10 = Linear(in_features, 31)\n",
    "        self.out = Linear(in_features, 5)\n",
    "    \n",
    "    def forward(self, x, input_dict):\n",
    "#         logit_label_10 = self.out_label_10(x)\n",
    "        logit = self.out(x)\n",
    "        loss = None\n",
    "        if input_dict['cal_loss']:\n",
    "#             loss_func = nn.CrossEntropyLoss().to(args.device)\n",
    "#             loss_label10 = loss_func(logit_label_10, input_dict['label_10'])\n",
    "            \n",
    "            loss_func = nn.CrossEntropyLoss(reduction='none').to(args.device)\n",
    "            loss_arr = loss_func(logit, input_dict['label'])\n",
    "            loss = (loss_arr * input_dict['sample_weight']).mean()\n",
    "        return loss, torch.nn.Softmax(1)(logit)\n",
    "\n",
    "class PoissonLossOut(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.out = Linear(in_features, 1)\n",
    "    \n",
    "    def forward(self, x, input_dict):\n",
    "        logit = self.out(x)\n",
    "        loss = None\n",
    "        if input_dict['cal_loss']:\n",
    "            loss_func = nn.PoissonNLLLoss(log_input=True, reduction='none')\n",
    "            max_v = 11\n",
    "            logit[logit>max_v] = max_v\n",
    "            loss_arr = loss_func(logit, input_dict['forword_cnt'].unsqueeze(-1)).squeeze(-1)\n",
    "            loss = (loss_arr * input_dict['sample_weight']).mean()        \n",
    "        return loss, torch.exp(logit)\n",
    "\n",
    "class MSELossOut(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.out = Linear(in_features, 1)\n",
    "    \n",
    "    def forward(self, x, input_dict):\n",
    "        global forword_cnt_mean, forword_cnt_std\n",
    "        logit = self.out(x)\n",
    "        loss = None\n",
    "        if input_dict['cal_loss']:\n",
    "            norm_target = (input_dict['forword_cnt']-forword_cnt_mean)/forword_cnt_std\n",
    "#             norm_target = torch.log(input_dict['forword_cnt'])\n",
    "            loss_arr = (logit.squeeze(-1)-norm_target)**2\n",
    "#             loss = (loss_arr * input_dict['sample_weight']).mean()\n",
    "            loss = loss_arr.mean()\n",
    "        \n",
    "        logit = logit*forword_cnt_std + forword_cnt_mean\n",
    "#         logit = torch.exp(logit)\n",
    "        logit[logit < 0] = 0\n",
    "        return loss, logit\n",
    "    \n",
    "class TargetLossOut(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.out = Linear(in_features, 5)\n",
    "    \n",
    "    def forward(self, x, input_dict):\n",
    "        global forword_cnt_mean, forword_cnt_std\n",
    "        logit = self.out(x)\n",
    "        loss = None\n",
    "        if input_dict['cal_loss']:\n",
    "            logit_softmax = torch.nn.Softmax(1)(logit) \n",
    "#             logit_softmax = torch.log(logit_softmax+0.001)\n",
    "            score_arr = logit_softmax * torch.tensor([0.00333333,0.03333333,0.16666667,0.33333333,1]).to(logit_softmax.device)\n",
    "            score_arr = score_arr[:, input_dict['label']] * torch.eye(input_dict['label'].shape[0]).to(logit_softmax.device)\n",
    "            loss = -torch.sum(score_arr)/torch.sum(input_dict['sample_weight'])\n",
    "            \n",
    "        return loss, logit \n",
    "    \n",
    "class SoftCeLossOut(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.out = Linear(in_features, 5)\n",
    "    \n",
    "    def forward(self, x, input_dict):\n",
    "        logit = self.out(x)\n",
    "        loss = None\n",
    "        logit = torch.nn.Softmax(1)(logit) \n",
    "\n",
    "        if input_dict['cal_loss']:\n",
    "            logit_softmax = torch.log(logit+0.001)\n",
    "#             score_arr = logit_softmax * torch.tensor([0.00333333,0.03333333,0.16666667,0.33333333,1]).to(logit_softmax.device)\n",
    "#             print(input_dict['soft_label'])\n",
    "#             print(score_arr.shape, input_dict['soft_label'].shape)\n",
    "            score_arr = logit_softmax * input_dict['soft_label']\n",
    "            score_arr = score_arr.sum(1) * input_dict['sample_weight']\n",
    "#             print(score_arr.shape)\n",
    "            loss = -torch.mean(score_arr)\n",
    "        return loss, logit\n",
    "\n",
    "class MLPAttentionPool(nn.Module):\n",
    "    def __init__(self,key_size,units):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(nn.Linear(key_size,units,bias=False),\n",
    "                                  nn.Tanh(),\n",
    "                                  nn.Linear(units,1,bias=False))\n",
    "        \n",
    "    def masked_softmax_1d(self, X, valid_len):\n",
    "        if valid_len is None:\n",
    "            return F.softmax(X,dim=-1), _\n",
    "        else:\n",
    "            shape=X.shape\n",
    "            if valid_len.dim()==1:\n",
    "                valid_len=valid_len.view(-1,1).repeat(1,shape[1])\n",
    "            \n",
    "            mask=(torch.arange(0,X.shape[-1]).to(X.device).repeat(X.shape[0],1)<valid_len).bool()\n",
    "            \n",
    "            X = X.masked_fill_(~mask, -float('inf'))\n",
    "            return F.softmax(X,dim=-1).view(shape), mask\n",
    "\n",
    "    def forward(self, key, valid_len):\n",
    "        scores = self.proj(key).squeeze(-1)\n",
    "        attention_weights, mask = self.masked_softmax_1d(scores,valid_len)\n",
    "        seq_out = attention_weights.unsqueeze(-1) * key\n",
    "        return seq_out.sum(1)\n",
    "    \n",
    "class RepostSetEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        user_id_embed = 10\n",
    "        text_embed = 10\n",
    "        user_dense_embed = 8\n",
    "        self.user_embeding = nn.Embedding(4000, user_id_embed)        \n",
    "        self.attention_pool = MLPAttentionPool(user_id_embed + text_embed + user_dense_embed, 64)\n",
    "        \n",
    "    def forward(self, input_dict):\n",
    "        x_set_wv = input_dict['repost_set_text_wv']\n",
    "        x_set_user_embed = self.user_embeding(input_dict['repost_set_userid'])\n",
    "        x_set_user_dense = input_dict['repost_user_dense']\n",
    "        x = torch.cat([x_set_wv, x_set_user_embed, x_set_user_dense], -1)\n",
    "        \n",
    "        return self.attention_pool(x, input_dict['repost_set_len'])\n",
    "    \n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        global continue_feature_list, one_hot_feature_list\n",
    "        \n",
    "        in_features = len(continue_feature_list)\n",
    "        bert_encode_feature = 312\n",
    "        bert_encode_feature = 0\n",
    "        text_wv_tfidf_feature = 40\n",
    "        user_id_embed = 10\n",
    "        repost_set_embed = 28\n",
    "#         repost_set_embed = 0\n",
    "        one_hot_feature = len(one_hot_feature_list)\n",
    "        \n",
    "#         global pretrained\n",
    "#         self.albert_weibotext = BertModel.from_pretrained(pretrained).to(args.device)\n",
    "#         self.albert_intro = BertModel.from_pretrained(pretrained).to(args.device)\n",
    "        \n",
    "#         self.transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=bert_encode_feature, nhead=8), \n",
    "#                                                     num_layers=1)\n",
    "\n",
    "        self.user_embeding = nn.Embedding(90, user_id_embed)\n",
    "        \n",
    "        self.repost_set_encoder = RepostSetEncoder().to(args.device)\n",
    "\n",
    "        self.fc = Linear(in_features, args.embedding_size, True)\n",
    "        \n",
    "        self.other_fc = Linear(2*bert_encode_feature+text_wv_tfidf_feature + user_id_embed + repost_set_embed + one_hot_feature,\n",
    "                         args.embedding_size, True)\n",
    "\n",
    "        self.fc_out = Linear(2*args.embedding_size, args.embedding_size, True)\n",
    "        \n",
    "        self.dp1 = torch.nn.Dropout(args.dropout_rate)\n",
    "        self.dp2 = torch.nn.Dropout(args.dropout_rate)\n",
    "\n",
    "        self.out_layer  = {\n",
    "                'ce': CeLossOut(args.embedding_size),\n",
    "                'poisson': PoissonLossOut(args.embedding_size),\n",
    "                'mse' : MSELossOut(args.embedding_size),\n",
    "                'target' : TargetLossOut(args.embedding_size),\n",
    "                'soft_ce':SoftCeLossOut(args.embedding_size),\n",
    "        }[args.loss]\n",
    "        \n",
    "                \n",
    "    def forward(self, input_dict):\n",
    "#         x_weibotext = self.albert_weibotext(input_dict['weibo_text_token'])[1]\n",
    "#         x_intro = self.albert_intro(input_dict['intro_text_token'])[1]\n",
    "#         x_seq = self.transformer_encoder(x_seq)\n",
    "#         x_text_out = torch.max(x_seq, 1)[0]\n",
    "#         print(x_bert.shape)\n",
    "        x_user_embed = self.user_embeding(input_dict['user_id_token'])\n",
    "        \n",
    "        x_repost_embeding = self.repost_set_encoder(input_dict)\n",
    "        \n",
    "        x = self.other_fc(torch.cat([input_dict['text_wv_tfidf'], x_user_embed,\n",
    "                       x_repost_embeding, input_dict['one_hot_feature']], 1))\n",
    "        x = torch.cat([x, self.fc(input_dict['x_dense'])], 1)\n",
    "        x = self.dp1(x)\n",
    "        x = self.fc_out(x)\n",
    "#         x = self.dp2(x)\n",
    "        return self.out_layer(x, input_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     5,
     32
    ]
   },
   "outputs": [],
   "source": [
    "'''\n",
    "评估函数传入的是1，2，3，4，5五个档位的值，注意从1开始\n",
    "\n",
    "'''\n",
    "\n",
    "def precision_score_hyr(predictions, ground_truths):\n",
    "    predictions, ground_truths = np.array(predictions)-1, np.array(ground_truths)-1\n",
    "    \n",
    "    score, total = 0, 0\n",
    "    for p_cnt, g_cnt in zip(predictions, ground_truths):\n",
    "        if g_cnt==0:\n",
    "            total += 1\n",
    "            if p_cnt == g_cnt:\n",
    "                score += 1\n",
    "        elif g_cnt==1:\n",
    "            total += 10\n",
    "            if p_cnt == g_cnt:\n",
    "                score += 10\n",
    "        elif  g_cnt==2:\n",
    "            total += 50\n",
    "            if p_cnt == g_cnt:\n",
    "                score += 50\n",
    "        elif  g_cnt==3:\n",
    "            total += 100\n",
    "            if p_cnt == g_cnt:\n",
    "                score += 100\n",
    "        elif  g_cnt==4:\n",
    "            total += 300\n",
    "            if p_cnt == g_cnt:\n",
    "                score += 300\n",
    "    return score/total\n",
    "\n",
    "def precision_score_cch(predictions, ground_truths):\n",
    "    \n",
    "    predictions, ground_truths = np.array(predictions)-1, np.array(ground_truths)-1\n",
    "    y_pred = predictions\n",
    "    y_true = ground_truths\n",
    "\n",
    "    w=[1,10,50,100,300]\n",
    "    n = len(y_true)\n",
    "    count_r = [0 for i in range(5)]\n",
    "    count = [0 for i in range(5)]\n",
    "    for i in range(n):\n",
    "        count[y_true[i]] += 1\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            count_r[y_pred[i]] += 1\n",
    "    sum1 = sum(w[i]*count_r[i] for i in range(5))\n",
    "    sum2 = sum(w[i]*count[i] for i in range(5))\n",
    "    precision = sum1/sum2\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_nn(weibo_id_list):\n",
    "    train_dataset=WeiboDataset(weibo_id_list)\n",
    "    data_loader = Data.DataLoader(\n",
    "        dataset=train_dataset,      \n",
    "        batch_size=args.batch_size,      \n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers = args.n_worker,\n",
    "    )\n",
    "    \n",
    "    model = DNN().to(args.device)\n",
    "        \n",
    "    no_decay = [\"bias\", \"gamma\",\"beta\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for name, p in model.named_parameters() if 'albert' not in name],\n",
    "            \"lr\": args.lr,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for name, p in model.named_parameters() if 'albert' in name],\n",
    "            \"lr\": args.fine_tune_layer_lr,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr = args.lr, weight_decay = args.weight_decay)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=int(len(weibo_id_list)//(args.batch_size)),\n",
    "        num_training_steps=int(len(weibo_id_list) / args.batch_size * args.epoch)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    for _ in range(args.epoch):\n",
    "        for input_dict in tqdm(data_loader):\n",
    "#             print(word_matrix)\n",
    "            loss, logit = model(input_dict)            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 5)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if args.debug:\n",
    "                break\n",
    "\n",
    "    return model\n",
    "\n",
    "def forward_nn(model, weibo_id_list):\n",
    "    model.eval()\n",
    "    train_dataset=WeiboDataset(weibo_id_list)\n",
    "    data_loader = Data.DataLoader(\n",
    "        dataset=train_dataset,      \n",
    "        batch_size=args.batch_size,      \n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x:collate_fn(x,False),\n",
    "        num_workers = args.n_worker,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        pre_list = []\n",
    "        for input_dict in tqdm(data_loader):\n",
    "            _, logit = model(input_dict)\n",
    "            pre_list.append(logit.cpu().detach().numpy())\n",
    "    return np.concatenate(pre_list)\n",
    "\n",
    "def cross_validation_nn(train_weibo_id_list, test_weibo_id_list):\n",
    "    n_flod = args.n_flod\n",
    "    folds = KFold(n_splits=n_flod, shuffle=True, random_state=SEED)\n",
    "    train_weibo_id_list = np.array(train_weibo_id_list)\n",
    "    test_weibo_id_list = np.array(test_weibo_id_list)\n",
    "    \n",
    "    \n",
    "    score_train = np.zeros((len(train_weibo_id_list), LOSS.out_feature_dim))\n",
    "    score_test = np.zeros((len(test_weibo_id_list), LOSS.out_feature_dim))\n",
    "    \n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_weibo_id_list, train_weibo_id_list)):\n",
    "        \n",
    "        trn_weibo_id_list = train_weibo_id_list[trn_idx]\n",
    "        val_weibo_id_list = train_weibo_id_list[val_idx]\n",
    "        model = train_nn(trn_weibo_id_list)\n",
    "        \n",
    "        model.eval()\n",
    "        trn_logit = forward_nn(model, trn_weibo_id_list)\n",
    "        val_logit = forward_nn(model, val_weibo_id_list)\n",
    "        \n",
    "#         print(trn_logit)\n",
    "#         print(pd.Series([count2idx(num)+1 for num in trn_logit[:, 0]]).value_counts())\n",
    "        \n",
    "        ipd.display(pd.DataFrame({\n",
    "            'train': [\n",
    "                precision_score_cch(LOSS.logit2count(trn_logit), \n",
    "                                    np.array(df_weibo.loc[trn_weibo_id_list]['label']).astype('int')+1)\n",
    "            ],\n",
    "            'val': [\n",
    "                precision_score_cch(LOSS.logit2count(val_logit), \n",
    "                                    np.array(df_weibo.loc[val_weibo_id_list]['label']).astype('int')+1),\n",
    "                     \n",
    "                ],\n",
    "        }))\n",
    "        \n",
    "        \n",
    "        score_train[val_idx] = val_logit\n",
    "\n",
    "        score_test += forward_nn(model, test_weibo_id_list)/n_flod\n",
    "\n",
    "    return score_train, score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:437: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a858569a5e74b1e92c39091caa556c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd385e99c364c49881d0f029b536223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c33a11c56e04913a7e64bfaaa3d4efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fafda24bddbe4ec1b83fcc70fedd5da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3e452124064459a17a1aaa948fa4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a4eb025a234ffdaae35741717db5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f6d7ed62a94557b8e2281a226a6d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.796319</td>\n",
       "      <td>0.664359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      train       val\n",
       "0  0.796319  0.664359"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65896c4ed4ed4b07a0f3af969efd12a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0b016f886149b5a9a6a092d418a6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec68dd5f5d14971851ae1e447a54ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5d6e9d0a28457dbab0532c605af843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c741afe4d2844831a1126edf92085830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fddb76fbf54ff58cf6a90e555dcac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d793075369b34814ae9ef41b8c76e601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07683ab604224897bdd3936e1ef83fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.801325</td>\n",
       "      <td>0.66658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      train      val\n",
       "0  0.801325  0.66658"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ac3ed494fd4d2c8a7e85adbfc15854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f7a872675348b6b9da688c38b85b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ac8efa95444cb0a52588489840c715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce46b0e9c2ba4f06925c9501e03f4a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1497db7e1d4c4b738a8d9cb9c6ee8235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00d4f2637cc4760aee31928bda1b318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bff00c8130d45f187ab9ea4d4d96fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1ee9b5a3724c21b46c7ba7bc4cc7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.796399</td>\n",
       "      <td>0.673493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      train       val\n",
       "0  0.796399  0.673493"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc638358caa419a881fd35bc6aacf1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941c9e8bc2e54d0b82afbeb66a50fdc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e2932d71ea474d97248ba89ea6d933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ba974d1d3e4d37b365bdf6afd5763d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b45fb3dd9e44058cf15e7255b3eab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2516d039f84d4415bd2e0fce15f901b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e64a3a9813243769ffddfaf2359a4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e8002a8f084544b88f7d0b2012fc41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.781428</td>\n",
       "      <td>0.687732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      train       val\n",
       "0  0.781428  0.687732"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bc858e141c46b4b087fa6548cb55fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cch/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177d67e573b240d6b2f68a17115f1873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbad9f545114e8d8ad305b155d76b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c201f673120542d1bc65204101233fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8428438b040b41fd823f484db6af5375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54976d2e0e8b41e789a49d91c715a415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dfd3ba1280448f0b60223f722d8ca1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de117357509243f6bef7c26d7bfbb0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.804927</td>\n",
       "      <td>0.664068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      train       val\n",
       "0  0.804927  0.664068"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9f8e072476401a84893cd0922e6b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-11 16:38:40,726 - INFO - here here here ce_0.671308\n"
     ]
    }
   ],
   "source": [
    "# for loss_name in ['ce', 'poisson', 'mse', 'target']:\n",
    "# for loss_name in ['ce', 'poisson', 'mse']:\n",
    "for loss_name in ['ce']:\n",
    "    ARG = namedtuple('ARG', [\n",
    "        'batch_size',\n",
    "        'epoch',\n",
    "        'lr',\n",
    "        'fine_tune_layer_lr',\n",
    "        'weight_decay',\n",
    "        'dropout_rate',\n",
    "        'n_worker',\n",
    "        'device',\n",
    "        'embedding_size',\n",
    "        'loss',\n",
    "        'weibo_text_max_length',\n",
    "        'n_flod',\n",
    "        'debug',\n",
    "    ])\n",
    "\n",
    "    args = ARG(\n",
    "        batch_size = 64,\n",
    "        epoch = 5,\n",
    "        lr = 0.006,\n",
    "        fine_tune_layer_lr=2e-5,\n",
    "        weight_decay = 0.01,\n",
    "        dropout_rate = 0.1,\n",
    "        n_worker = 0,\n",
    "        device=torch.device(\"cuda:0\"),\n",
    "    #     device=torch.device(\"cpu\"),\n",
    "        embedding_size = 100,\n",
    "        loss = loss_name, #ce poisson mse target,\n",
    "        weibo_text_max_length = 500,\n",
    "        n_flod = 5,\n",
    "        debug = False\n",
    "    )\n",
    "\n",
    "    def setup_seed(seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    # 设置随机数种子\n",
    "    SEED = 42\n",
    "    setup_seed(SEED)\n",
    "\n",
    "    def count2idx(num):\n",
    "        if 0 <= num <= 10:\n",
    "            return 0\n",
    "        elif 10 <= num <= 50:\n",
    "            return 1\n",
    "        elif 50 <= num <= 150:\n",
    "            return 2\n",
    "        elif 150 <= num <= 300:\n",
    "            return 3\n",
    "        elif num >= 300:\n",
    "            return 4\n",
    "        if num < 0:\n",
    "            print(num)\n",
    "            return 0\n",
    "        print(num)\n",
    "\n",
    "    LOSS_PARAMETER = namedtuple('LOSS_PARAMETER',[\n",
    "        'out_feature_dim',\n",
    "        'logit2count',\n",
    "    ])\n",
    "\n",
    "    LOSS = LOSS_PARAMETER(\n",
    "        out_feature_dim={\n",
    "            'ce':5,\n",
    "            'poisson':1,\n",
    "            'mse':1,\n",
    "            'target':5,\n",
    "            'soft_ce':5\n",
    "        }[args.loss],\n",
    "\n",
    "        logit2count={\n",
    "            'ce':lambda logit: np.argmax(logit,1)+1,\n",
    "            'poisson':lambda logit:[count2idx(num)+1 for num in logit[:, 0]],\n",
    "            'mse':lambda logit:[count2idx(num)+1 for num in logit[:, 0]],\n",
    "            'target':lambda logit: np.argmax(logit,1)+1,\n",
    "            'soft_ce':lambda logit: np.argmax(logit,1)+1,\n",
    "        }[args.loss]\n",
    "    )\n",
    "\n",
    "    pretrained = '/data/cch/hyr/weibo/voidful/albert_chinese_tiny'\n",
    "    albert_tokenizer = BertTokenizer.from_pretrained(pretrained)\n",
    "    # albert_model = BertModel.from_pretrained(pretrained).to(args.device)\n",
    "    start = time.time()\n",
    "    train_weibo_id_list = list(df_weibo.query('type == \"train\"').index)\n",
    "    test_weibo_id_list = list(df_weibo.query('type == \"test\"').index)\n",
    "\n",
    "    score_train, score_test =\\\n",
    "    cross_validation_nn(train_weibo_id_list, test_weibo_id_list)\n",
    "\n",
    "    time.time() - start\n",
    "\n",
    "    MODEL_OUT_PATH = '/data/cch/hyr/weibo/sub_model_output'\n",
    "\n",
    "    ret_dict = {'WeiboId':train_weibo_id_list+test_weibo_id_list}\n",
    "    ret_dict.update(\n",
    "        dict([['nn_hyr_loss_%s_%d'%(args.loss, i), np.concatenate([score_train[:, i], score_test[:, i]])] \\\n",
    "              for i in range(LOSS.out_feature_dim)])\n",
    "    )\n",
    "    df_model_output = pd.DataFrame(ret_dict)\n",
    "\n",
    "#     pickle.dump(df_model_output, open('%s/df_nn_loss_%s.pickle'%(MODEL_OUT_PATH, args.loss), 'wb'))\n",
    "#     df_model_output = pickle.load(open('%s/df_nn_loss_%s.pickle'%(MODEL_OUT_PATH, args.loss), 'rb'))\n",
    "\n",
    "    logit_feature_name = sorted(list( set(df_model_output.columns) - set(['WeiboId'])))\n",
    "    score = precision_score_cch(LOSS.logit2count(np.array(df_model_output.iloc[:18000][logit_feature_name])), \n",
    "                       np.array(df_weibo.loc[train_weibo_id_list]['label']).astype('int')+1)\n",
    "    \n",
    "    logging.info('here here here %s_%f'%(loss_name, score))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df_model_output, open('%s/df_nn_loss_%s.pickle'%(MODEL_OUT_PATH, args.loss), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(np.array(df_model_output.iloc[:18000][logit_feature_name]), 1)\n",
    "lables = df_weibo.iloc[:18000]['label']\n",
    "forward_cnt = df_weibo.iloc[:18000]['ForwordCount']\n",
    "\n",
    "df_analysis = pd.DataFrame({\n",
    "    'WeiboId':df_weibo.iloc[:18000].index,\n",
    "    'predictions':predictions,\n",
    "    'lables':list(lables),\n",
    "    'forward_cnt':list(forward_cnt)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "0.675909"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "569.6px",
    "left": "26px",
    "top": "166px",
    "width": "285px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
